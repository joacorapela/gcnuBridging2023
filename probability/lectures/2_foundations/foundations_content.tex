
\mode<presentation> {

% The Beamer class comes with a number of default slide themes
% which change the colors and layouts of slides. Below this is a list
% of all the themes, uncomment each in turn to see what they look like.

%\usetheme{default}
%\usetheme{AnnArbor}
%\usetheme{Antibes}
%\usetheme{Bergen}
%\usetheme{Berkeley}
%\usetheme{Berlin}
%\usetheme{Boadilla}
%\usetheme{CambridgeUS}
% \usetheme{Copenhagen}
%\usetheme{Darmstadt}
%\usetheme{Dresden}
%\usetheme{Frankfurt}
%\usetheme{Goettingen}
%\usetheme{Hannover}
%\usetheme{Ilmenau}
%\usetheme{JuanLesPins}
%\usetheme{Luebeck}
%\usetheme{Madrid}
%\usetheme{Malmoe}
%\usetheme{Marburg}
%\usetheme{Montpellier}
\usetheme{PaloAlto}
%\usetheme{Pittsburgh}
%\usetheme{Rochester}
%\usetheme{Singapore}
%\usetheme{Szeged}
%\usetheme{Warsaw}

% As well as themes, the Beamer class has a number of color themes
% for any slide theme. Uncomment each of these in turn to see how it
% changes the colors of your current slide theme.

%\usecolortheme{albatross}
%\usecolortheme{beaver}
%\usecolortheme{beetle}
%\usecolortheme{crane}
%\usecolortheme{dolphin}
%\usecolortheme{dove}
%\usecolortheme{fly}
%\usecolortheme{lily}
%\usecolortheme{orchid}
%\usecolortheme{rose}
%\usecolortheme{seagull}
%\usecolortheme{seahorse}
%\usecolortheme{whale}
%\usecolortheme{wolverine}

%\setbeamertemplate{footline} % To remove the footer line in all slides uncomment this line
%\setbeamertemplate{footline}[page number] % To replace the footer line in all slides with a simple slide count uncomment this line

%\setbeamertemplate{navigation symbols}{} % To remove the navigation symbols from the bottom of all slides uncomment this line
}

\hypersetup{colorlinks,citecolor=}
\usepackage{graphicx} % Allows including images
\usepackage{booktabs} % Allows the use of \toprule, \midrule and \bottomrule in tables
\usepackage{natbib}
\usepackage{apalike}
\usepackage{comment}
% \usepackage{enumitem}
% \setlist[itemize]{topsep=0pt,before=\leavevmode\vspace{-1.5em}}
% \setlist[description]{style=nextline}
\usepackage{amsthm}
\usepackage{media9}
% \usepackage{multimedia}

\newtheorem{probDef}{Definition}
\newtheorem*{probDef*}{Definition}
\newtheorem{claim}{Claim}
\newtheorem{probExample}{Example}
\newtheorem{probRule}{Rule}
\newtheorem{probAxiom}{Axiom}
\setbeamertemplate{theorems}[numbered]

\newtheorem{manualprobRuleinner}{Rule}
\newenvironment{manualProbRule}[1]{%
  \renewcommand\themanualprobRuleinner{#1}%
  \manualprobRuleinner
}{\endmanualprobRuleinner}

%----------------------------------------------------------------------------------------
%	TITLE PAGE
%----------------------------------------------------------------------------------------

\title{Foundations of probability theory}

\author{Joaqu\'{i}n Rapela} % Your name
\institute[GCNU, UCL] % Your institution as it will appear on the bottom of every slide, may be shorthand to save space
{
Gatsby Computational Neuroscience Unit\\University College London % Your institution for the title page
}
\date{\today} % Date, can be changed to a custom date

\AtBeginSection[]
  {
     \begin{frame}<beamer>
     \frametitle{Contents}
         \tableofcontents[currentsection,hideallsubsections]
     \end{frame}
  }

\begin{document}

\begin{frame}
\titlepage % Print the title page as the first slide
\end{frame}

\begin{frame}
\frametitle{Contents} % Table of contents slide, comment this block out to remove it
\tableofcontents % Throughout your presentation, if you choose to use \section{} and \subsection{} commands, these will automatically be printed on this slide as an overview of your presentation
\end{frame}

\begin{frame}
\frametitle{Main reference} % Table of contents slide, comment this block out to remove it

    I will mainly follow chapters seven \textit{Foundations of probability
    theory} and eight \textit{Conditional probability and Bayes} from
    \citet{tijms12}.

\end{frame}

\section{Foundations of probability theory}

\subsection{Historical notes}

\begin{frame}
\frametitle{Historical notes}

- explain the frequency-based interpretation of probability.

- constructing the mathematical foundations of probability theory has proven to be a long-lasting process of trial an error.  

- the approach of defining probability as relative frequencies of repeatable experiments lead to unsatisfactory theory (why?)
https://www.jstor.org/stable/pdf/20115155.pdf

- the frequency view of probability has a long history that goes back to Aristotle.

- in 1933 the Russian mathematician Andrej Kolmogrov (1903-1987) laid a satisfactory mathematical foundation of probability theory.

He created a set of axioms. Axioms state a number of minimal requirements that the probability objects should satisfy. From these few algorithms all claims of probability can be derived, as we will see.

\end{frame}

\subsection{Axiomatic definition of probability}

\begin{frame}
\frametitle{Probability model}

    \begin{probDef}[Probability model]
        A \textbf{probability model} is a matematical representation of a
        random experiment. \onslide<2->{It consists of a description of all possible
        outcomes of the experiment (i.e., \textbf{sample space}), a set of
        subsets of the sample space (i.e., \textbf{events}), and an assigment of
        probability to events (i.e., \textbf{probability measure}).}
        \label{def:probabilityModel}
    \end{probDef}

\end{frame}

\begin{frame}
    \frametitle{Sample space}

    \begin{probDef}[Sample space]
        The set of all samples in an experiment is called the \textbf{sample
        space}. It is denoted by $\Omega$.
    \end{probDef}

\end{frame}

\begin{frame}
    \frametitle{Event}

    \begin{probDef}[Event]

        An \textbf{event} is a subset of the sample space. We denote the
        collection of all events by $\mathcal{F}$.

    \end{probDef}

    \onslide<2->{

    Notes:
    \begin{enumerate}

        \item We will only assign probabilities to events (i.e., to sets
            $A\in\mathcal{F}$).

        \item For finite or countable sample spaces, we can assign
            probabilities to any subset of the sample space. Thus, any subset
            of a finite or countable sample space can be an event.

        \item For uncountable sample spaces, we can only assign probabilities
            to well behaved subsets of the sample space (i.e., to elements in a
            $\sigma$ algebra of subsets of the sample space). Only well-behaved
            subsets of an uncountable sample space can be events.

    \end{enumerate}
    }

\end{frame}

\begin{frame}
    \frametitle{Probability measure}

    \begin{probDef}[Probability measure]

        A \textbf{probability measure} is a function that assigns numbers
        between zero and one to events (i.e., $P:\mathcal{F}\rightarrow
        [0,1]$).

    \end{probDef}

\end{frame}

\begin{frame}
    \frametitle{Probability model: definitions}

    \begin{probDef*}[Sample space]

        The set of all samples in an experiment is called the \textbf{sample
        space}. It is denoted by $\Omega$.

    \end{probDef*}


    \footnotesize
    \begin{probDef*}[Event]

        An \textbf{event} is a subset of the sample space. We denote the
        collection of all events by $\mathcal{F}$.

    \end{probDef*}

    \begin{probDef*}[Probability measure]

        A \textbf{probability measure} is a function that assigns numbers
        between zero and one to events (i.e., $P:\mathcal{F}\rightarrow
        [0,1]$).

    \end{probDef*}

    \begin{probDef*}[Probability model]

        A \textbf{probability model}, $\mathcal{M}$, is a mathemtatical representation of a
        random experiment consiting of a sample space, $\Omega$, a set of
        events, $\mathcal{F}$, and a probability measure, $P$ (i.e.,
        $\mathcal{M}=\{\Omega,\mathcal{F},P\}$).

    \end{probDef*}
    \normalsize

\end{frame}

\begin{frame}
    \frametitle{Examples of probability models}

    \small
    For each of the following examples, let's find the sample space and propose
    a probability measure.

    \begin{enumerate}

        \item The experiment is to toss a fair coin once. \onslide<2->{The
            sample space is the set [\textit{H}, \textit{T}].} \onslide<3->{We
            assign a probability of 0.5 to each element of the sample space.}

        \onslide<4->{\item The experiment is to repeately roll a fair dice and
            count the number of rolls until the first six shows up.}
            \onslide<5->{The sample space is the set of positive integers.}
            \onslide<6->{The probabilities
            $\frac{1}{6},\frac{5}{6}\times\frac{1}{6},\left(\frac{5}{6}\right)^2\times\frac{1}{6},
            \ldots$ can be assigned to the outcomes $1,2,3,\ldots$.}

        \item \onslide<7->{The experiment is to measure the time between the
            first and second spikes in an experimental trial.} \onslide<9->{The
            sample space is the set $(0,\infty)$ of positive real numbers.}
            \onslide<9->{We can assign a probability of $1-\exp(-\lambda t)$ to
            the event that the second spike is fired less than $t$ seconds
            after the first spike.}

    \end{enumerate}
    \normalsize

    \note{

        - highlight that we discussed finite, countable and uncountable sample
        spaces.

        - Cantor (1845-1918): the set of all real numbers, and the set of real
        numbers between zero and one, are examples of infinite, and
        uncountable, sample spaces.

        - mention that the probability of a sample in an uncountable sample
        space is zero.

    }

\end{frame}

\begin{frame}
\frametitle{Axioms of probability theory}


    \begin{probAxiom}

        $P(A)\ge 0,\quad\forall A\in\mathcal{F}$

    \end{probAxiom}

    \begin{probAxiom}

        $P(\Omega)=1$

    \end{probAxiom}

    \begin{probAxiom}

        $P\left(\bigcup_{i=1}^\infty A_i\right)=\sum_{i=1}^\infty P(A_i)$ for every collection of pairwise disjoint events $A_1,A_2,\ldots$

    \end{probAxiom}


    \note{

        - explain what an infinite union means

        - For a finite or countable sample space it suffices that

            - $P(\omega)\ge 0$
            - $\sum_{\omega\in\Omega}P(\omega)=1$

    }

\end{frame}

\begin{frame}
    \frametitle{Experiment with equally likely outcomes}

    An experiment with equally likely outcomes is one with a finite number of
    outcomes $\omega_1,\ldots,\omega_N$, where all outcomes have the same
    probability (i.e., $P(\omega_i)=\frac{1}{N}$).

    \onslide<2->{

    \begin{claim}
        For any event $A$, $P(A)=\frac{N(A)}{N}$, where $N(A)$ is the number of
        outcomes in the set $A$.
    \end{claim}

    }

\end{frame}

\begin{frame}
    \frametitle{Example: equally likely outcomes -- discrete $\Omega$}

    \begin{probExample}
        John, Pedro and Rosita each roll on fair die. How do we calculate the
        probability that the score of Rosita is equal to the sum of the scores
        of John and Pedro?
    \end{probExample}

    \note{

        - examples

        - uncountable sample space

    }

\end{frame}

\subsection{Some basic rules}

\begin{frame}
    \frametitle{Some basic rules}

    \small
    \begin{probRule}

        For any finite number of mutually exclusive events $A_1,\ldots,A_N$,
        \begin{align*}
            P(A_1\cup A_2\cup\ldots\cup A_n) = P(A_1) + \ldots + P(A_N)
        \end{align*}

    \end{probRule}

    \begin{probRule}

        For any event A,
        \begin{align*}
            P(A) = 1 - P(A^c)
        \end{align*}
        where the event $A^c$ consists of all outcomes that are not in $A$.

    \end{probRule}

    \begin{probRule}

        For any two events A and B,
        \begin{align*}
            P(A\cup B) = P(A) + P(B) - P(A\cap B)
        \end{align*}

    \end{probRule}
    \normalsize
\end{frame}

\begin{frame}
    \frametitle{Rule 1}

	\footnotesize
	\begin{proof}
		Denote by $\emptyset$ the empty set. We will firt prove that
        $P(\emptyset)=0$. Take $A_i=\emptyset$ for $i=1, 2, \ldots$. Then
        $\emptyset=\bigcup_{i=1}^\infty A_i$. Next, by Axiom~3, $P(\emptyset)=\sum_{i=1}^\infty P(A_i)=\sum_{i=1}^\infty P(\emptyset)$. This implies that $P(\emptyset)=0$.

		Define $A_{N+j}=\emptyset$ for $j=1,2,\ldots$. Then
		\footnotesize
		\begin{align*}
            P(A_1\cup A_2\cup\ldots\cup A_N)&=P(A_1\cup A_2\cup\ldots\cup A_N\cup A_{N+1}\cup A_{N+2}\cup\ldots)\\
										    &=P(\bigcup_{i=1}^\infty A_i)=\sum_{i=1}^\infty P(A_i)\\
                                            &=\sum_{i=1}^N P(A_i)+\sum_{j=1}^\infty P(A_{N+j})=\sum_{i=1}^N P(A_i)
		\end{align*}
		\normalsize

	\end{proof}

	Notes:
	\begin{enumerate}
		\item the last equality in the second line holds by Axiom~1
		\item the last equality in the third line holds because $P(A_{N+j})=P(\emptyset)=0$.
	\end{enumerate}
    \normalsize

\end{frame}

\begin{frame}
    \frametitle{Proof of rule 2}

	\begin{proof}

		$\Omega=A\cup A^c$. $A$ and $A^c$ are disjoint. Then, by Rule~1, $P(\Omega)=P(A)+P(A^c)$. From Axiom~2, $P(\Omega)=1$. Thus, $1=P(A)+P(A^c)$.

	\end{proof}

\end{frame}

\begin{frame}
    \frametitle{Proof of rule 3}
    \tiny
	\begin{proof}
		\begin{align*}
            A\cup B&=\left(A\setminus B\right)\cup\left(B\setminus A\right)\cup\left(A\cap B\right)\\
            A&=\left(A\setminus B\right)\cup \left(A\cap B\right)\\
            B&=\left(B\setminus A\right)\cup \left(A\cap B\right)
		\end{align*}
        Since the sets in the right-hand-side of the above equations are
        pairwise disjoint, by rule 1, we obtain
		\begin{align*}
            P\left(A\cup B\right)&=P\left(A\setminus B\right)+P\left(B\setminus A\right)+P\left(A\cap B\right)\\
            P\left(A\right)&=P\left(A\setminus B\right)+P\left(A\cap B\right)\rightarrow P\left(A\setminus B\right)=P\left(A\right)-P\left(A\cap B\right)\\
            P\left(B\right)&=P\left(B\setminus A\right)+P\left(A\cap B\right)\rightarrow P\left(B\setminus A\right)=P\left(B\right)-P\left(A\cap B\right)
		\end{align*}
        Replazing the equations on the right of the second and third line in
        the equation on the first line the rule is proved.
		\begin{align*}
            P\left(A\cup B\right)&=P\left(A\right)-P\left(A\cap B\right)+P\left(B\right)-P\left(A\cap B\right)+P\left(A\cap B\right)\\
                                 &=P\left(A\right)+P\left(B\right)-P\left(A\cap B\right)
		\end{align*}
	\end{proof}
    \normalsize
\end{frame}

\begin{frame}
    \frametitle{Example: Chevalier de Mere to Blaise Pascal}

    The gambler Chevalier de Mere posed the following problem to the famous
    French mathematician Blaise Pascal in 1654. This problem marks the begining
    of probability theory.

    \begin{probExample}
        How many rolls of a fair die are required to have at least a
        50\% chance of rolling at least one six? How many rolls of two fair
        dice are required to hoave at least a 50\% chance of rolling at least
        one double six?
    \end{probExample}

\end{frame}

\begin{frame}
    \frametitle{Example:}

    - example 7.8 (rule 7-3, addition rule, easy)

\end{frame}

\begin{frame}
    \frametitle{Example:}

    - example 7.9 (rule 7-3): uses counting tools (binomial coefficient)

        - wrong, but simple, approach

        - correct, but more complicated, approach

        - sampling approach

\end{frame}

\begin{frame}
    \frametitle{Example:}

    - example 7.10 (rule 7-1, birthday problem, used in example 8.6): uses counting tools (binomial coefficient)

\end{frame}

\section{Conditional probability and Bayes}

\subsection{Conditional probability}

\begin{frame}
    \frametitle{Conditional probability}

- p. 256: good motivation of conditional probability in the cards example

- Definition 8.1

- interpretation of condition probability with relative frequencies

\end{frame}

\begin{frame}
    \frametitle{Example:}

- Example 8.1 (first ask students their intuition, as the problem is counter intuitive)

- do NOT present example 8.2 at this point, as it requires the concept of independence

\end{frame}

\subsection{Assigning probabilities by conditioning}

\begin{frame}
    \frametitle{Assigning probabilities by conditioning}

    \begin{description}

        \item[Rule 4] For any sequence of events $A_1,\ldots,A_n$,

            \begin{align*}
                P(A_1, ..., A_n) = P(A_n|A_{n-1}, ..., A_1) \ldots P(A_1)
            \end{align*}

    \end{description}

\end{frame}

\begin{frame}
    \frametitle{Example:}

    - redo Example 7.9 (solution following Rule 4)

\end{frame}

\begin{frame}
    \frametitle{Example:}

    - probability that it takes 10 or more cards before the first ace appears

\end{frame}

\subsection{Independent events}

\begin{frame}
    \frametitle{Independent events}

- motivation of independence definition with conditional probabilities

- Definition 8.2

\end{frame}

\begin{frame}
    \frametitle{Example}

- Example 8.5

\end{frame}

\begin{frame}
    \frametitle{Example}

- Example 8.6 (uses birthday problem, example 7.10)

\end{frame}

\subsection{Law of conditional probability}

\begin{frame}
    \frametitle{Law of conditional probability}

- example of dice followed by coin tosses

    \begin{description}

        \item{Rule 5} law of conditional probability. Let $A$ be an event that
            can only occur if one of the mutually exclusive events
            $B_1,\ldots,B_n$ occurs. Then

            \begin{align*}
                P(A) = P(A|B_1) P(B_1) + \ldots + P(A|B_n) P(B_n)  
            \end{align*}

    \end{description}

\end{frame}

\begin{frame}
    \frametitle{Example}

- example 8.6: tour the France (difficult!)

\end{frame}

\subsection{Baye's rule in odds form}

\begin{frame}
    \frametitle{Bayes rule in odds form}

- true/false hypothesis

    \begin{description}

        \item{Rule 6} The posterior probability $P(H|E)$ satisfies

            \begin{align*}
                \frac{P(H|E)}{P(\bar{H}|E)} = \frac{P(H)}{P(\bar{H})}\frac{P(E|H)}{P(E|\bar{H})}
            \end{align*}

    \end{description}

- interpretation of rule 6

    - avoid need of P(E)

    - prior odds + likelihood ratio or Bayes factor

    - prior odds update with new evidence

    - sequential update (mention Bayesian linear regression)

\end{frame}

\begin{frame}
    \frametitle{Example}

- example 8.8

\end{frame}

\begin{frame}
    \frametitle{Example}

- example 8.11

    - add to the problem statement:

        - in 1992, 4936 women were murdered in the US, of which roughly 1430 were murdered by their (ex)husbands or boyfriends

        - 5\% of the married women in the US have at some point been physically abused by their husbands.

        - assume that a woman who has been murdered by some other than her husband had the same same chance of being abused by her husband as a randomly selected woman

        - Alan Dershowitz admitted that a substantial percentage of the husbands who murder their wives, previous to the murder, also physically abuse their wives. Given this statement, we assume that the proability that a husband physically abused his wife, given that he killed her, is 50 percent.

\end{frame}

\subsection{Bayesian inference -- discrete case}

\begin{frame}
    \frametitle{Bayesian inference -- discrete case}

- explain posterior sequential update

\end{frame}

\begin{frame}
    \frametitle{Example}

- example 8.13 (solve it analytically and by sampling)

\end{frame}

\begin{frame}
    \frametitle{References}

    \tiny{
        \bibliographystyle{apalike}
        \bibliography{probability}
    }
\end{frame}

\begin{comment}
\end{comment}

\end{document}

