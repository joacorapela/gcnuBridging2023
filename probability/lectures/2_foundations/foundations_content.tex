
\mode<presentation> {

% The Beamer class comes with a number of default slide themes
% which change the colors and layouts of slides. Below this is a list
% of all the themes, uncomment each in turn to see what they look like.

%\usetheme{default}
%\usetheme{AnnArbor}
%\usetheme{Antibes}
%\usetheme{Bergen}
%\usetheme{Berkeley}
%\usetheme{Berlin}
%\usetheme{Boadilla}
%\usetheme{CambridgeUS}
%\usetheme{Copenhagen}
%\usetheme{Darmstadt}
%\usetheme{Dresden}
%\usetheme{Frankfurt}
%\usetheme{Goettingen}
%\usetheme{Hannover}
%\usetheme{Ilmenau}
%\usetheme{JuanLesPins}
%\usetheme{Luebeck}
%\usetheme{Madrid}
%\usetheme{Malmoe}
%\usetheme{Marburg}
%\usetheme{Montpellier}
\usetheme{PaloAlto}
%\usetheme{Pittsburgh}
%\usetheme{Rochester}
%\usetheme{Singapore}
%\usetheme{Szeged}
%\usetheme{Warsaw}

% As well as themes, the Beamer class has a number of color themes
% for any slide theme. Uncomment each of these in turn to see how it
% changes the colors of your current slide theme.

%\usecolortheme{albatross}
%\usecolortheme{beaver}
%\usecolortheme{beetle}
%\usecolortheme{crane}
%\usecolortheme{dolphin}
%\usecolortheme{dove}
%\usecolortheme{fly}
%\usecolortheme{lily}
%\usecolortheme{orchid}
%\usecolortheme{rose}
%\usecolortheme{seagull}
%\usecolortheme{seahorse}
%\usecolortheme{whale}
%\usecolortheme{wolverine}

%\setbeamertemplate{footline} % To remove the footer line in all slides uncomment this line
%\setbeamertemplate{footline}[page number] % To replace the footer line in all slides with a simple slide count uncomment this line

%\setbeamertemplate{navigation symbols}{} % To remove the navigation symbols from the bottom of all slides uncomment this line
}

% remove title and author from left panel
 \makeatletter
  \setbeamertemplate{sidebar \beamer@sidebarside}%{sidebar theme}
  {
    \beamer@tempdim=\beamer@sidebarwidth%
    \advance\beamer@tempdim by -6pt%
    \insertverticalnavigation{\beamer@sidebarwidth}%
    \vfill
    \ifx\beamer@sidebarside\beamer@lefttext%
    \else%
      \usebeamercolor{normal text}%
      \llap{\usebeamertemplate***{navigation symbols}\hskip0.1cm}%
      \vskip2pt%
    \fi%
  }%
\makeatother
% done remove title and author from left panel 

\hypersetup{colorlinks,citecolor=}
\usepackage{graphicx} % Allows including images
\usepackage{booktabs} % Allows the use of \toprule, \midrule and \bottomrule in tables
\usepackage{natbib}
\usepackage{apalike}
\usepackage{comment}
% \usepackage{enumitem}
% \setlist[itemize]{topsep=0pt,before=\leavevmode\vspace{-1.5em}}
% \setlist[description]{style=nextline}
\usepackage{amsthm}
\usepackage{media9}
% \usepackage{multimedia}
\usepackage{hyperref}
\usepackage{tikz}
\tikzset{
     arrow/.style={-{Stealth[]}}
     }
\usetikzlibrary{positioning,arrows.meta}

\setbeamertemplate{navigation symbols}{}%remove navigation symbols

\usepackage{setspace}

\newtheorem{probDef}{Definition}
\newtheorem*{probDef*}{Definition}
\newtheorem{claim}{Claim}
\newtheorem{probExample}{Example}
\newtheorem{probRule}{Rule}
\newtheorem{probAxiom}{Axiom}
\setbeamertemplate{theorems}[numbered]

\newtheorem{manualprobRuleinner}{Rule}
\newenvironment{manualProbRule}[1]{%
  \renewcommand\themanualprobRuleinner{#1}%
  \manualprobRuleinner
}{\endmanualprobRuleinner}

\newtheorem{manualprobExampleinner}{Example}
\newenvironment{manualProbExample}[1]{%
  \renewcommand\themanualprobExampleinner{#1}%
  \manualprobExampleinner
}{\endmanualprobExampleinner}

%----------------------------------------------------------------------------------------
%	TITLE PAGE
%----------------------------------------------------------------------------------------

\title{Foundations of probability theory}

\author{Joaqu\'{i}n Rapela} % Your name
\institute[GCNU, UCL] % Your institution as it will appear on the bottom of every slide, may be shorthand to save space
{
Gatsby Computational Neuroscience Unit\\University College London % Your institution for the title page
}
\date{\today} % Date, can be changed to a custom date

\AtBeginSection[]
  {
     \begin{frame}<beamer>
     \frametitle{Contents}
         \tableofcontents[currentsection,hideallsubsections]
     \end{frame}
  }

\begin{document}

\begin{frame}
\titlepage % Print the title page as the first slide
\end{frame}

\begin{frame}
\frametitle{Contents} % Table of contents slide, comment this block out to remove it
\tableofcontents % Throughout your presentation, if you choose to use \section{} and \subsection{} commands, these will automatically be printed on this slide as an overview of your presentation
\end{frame}

\begin{frame}
\frametitle{Main reference} % Table of contents slide, comment this block out to remove it

    I will mainly follow chapters seven \textit{Foundations of probability
    theory} and eight \textit{Conditional probability and Bayes} from
    \citet{tijms12}.

\end{frame}

\section{Foundations of probability theory}

\subsection{Historical notes}

\begin{frame}
\frametitle{Historical notes}

- explain the frequency-based interpretation of probability.

- constructing the mathematical foundations of probability theory has proven to be a long-lasting process of trial an error.  

- the approach of defining probability as relative frequencies of repeatable experiments lead to unsatisfactory theory (why?)
https://www.jstor.org/stable/pdf/20115155.pdf

- the frequency view of probability has a long history that goes back to Aristotle.

- in 1933 the Russian mathematician Andrej Kolmogrov (1903-1987) laid a satisfactory mathematical foundation of probability theory.

He created a set of axioms. Axioms state a number of minimal requirements that the probability objects should satisfy. From these few algorithms all claims of probability can be derived, as we will see.

\end{frame}

\subsection{Axiomatic definition of probability}

\begin{frame}
\frametitle{Probability model}

    \begin{probDef}[Probability model]
        A \textbf{probability model} is a matematical representation of a
        random experiment. \onslide<2->{It consists of a description of all possible
        outcomes of the experiment (i.e., \textbf{sample space}), a set of
        subsets of the sample space (i.e., \textbf{events}), and an assigment of
        probability to events (i.e., \textbf{probability measure}).}
        \label{def:probabilityModel}
    \end{probDef}

\end{frame}

\begin{frame}
    \frametitle{Sample space}

    \begin{probDef}[Sample space]
        The set of all samples in an experiment is called the \textbf{sample
        space}. It is denoted by $\Omega$.
    \end{probDef}

\end{frame}

\begin{frame}
    \frametitle{Event}

    \begin{probDef}[Event]

        An \textbf{event} is a subset of the sample space. We denote the
        collection of all events by $\mathcal{F}$.

    \end{probDef}

    \onslide<2->{

    Notes:
    \begin{enumerate}

        \item We will only assign probabilities to events (i.e., to sets
            $A\in\mathcal{F}$).

        \item For finite or countable sample spaces, we can assign
            probabilities to any subset of the sample space. Thus, any subset
            of a finite or countable sample space can be an event.

        \item For uncountable sample spaces, we can only assign probabilities
            to well behaved subsets of the sample space (i.e., to elements in a
            $\sigma$ algebra of subsets of the sample space). Only well-behaved
            subsets of an uncountable sample space can be events.

    \end{enumerate}
    }

\end{frame}

\begin{frame}
    \frametitle{Probability measure}

    \begin{probDef}[Probability measure]

        A \textbf{probability measure} is a function that assigns numbers
        between zero and one to events (i.e., $P:\mathcal{F}\rightarrow
        [0,1]$).

    \end{probDef}

\end{frame}

\begin{frame}
    \frametitle{Probability model: definitions}

    \begin{probDef*}[Sample space]

        The set of all samples in an experiment is called the \textbf{sample
        space}. It is denoted by $\Omega$.

    \end{probDef*}


    \footnotesize
    \begin{probDef*}[Event]

        An \textbf{event} is a subset of the sample space. We denote the
        collection of all events by $\mathcal{F}$.

    \end{probDef*}

    \begin{probDef*}[Probability measure]

        A \textbf{probability measure} is a function that assigns numbers
        between zero and one to events (i.e., $P:\mathcal{F}\rightarrow
        [0,1]$).

    \end{probDef*}

    \begin{probDef*}[Probability model]

        A \textbf{probability model}, $\mathcal{M}$, is a mathemtatical representation of a
        random experiment consiting of a sample space, $\Omega$, a set of
        events, $\mathcal{F}$, and a probability measure, $P$ (i.e.,
        $\mathcal{M}=\{\Omega,\mathcal{F},P\}$).

    \end{probDef*}
    \normalsize

\end{frame}

\begin{frame}
    \frametitle{Examples of probability models}

    \small
    For each of the following examples, let's find the sample space and propose
    a probability measure.

    \begin{enumerate}

        \item The experiment is to toss a fair coin once. \onslide<2->{The
            sample space is the set [\textit{H}, \textit{T}].} \onslide<3->{We
            assign a probability of 0.5 to each element of the sample space.}

        \onslide<4->{\item The experiment is to repeately roll a fair die and
            count the number of rolls until the first six shows up.}
            \onslide<5->{The sample space is the set of positive integers.}
            \onslide<6->{The probabilities
            $\frac{1}{6},\frac{5}{6}\times\frac{1}{6},\left(\frac{5}{6}\right)^2\times\frac{1}{6},
            \ldots$ can be assigned to the outcomes $1,2,3,\ldots$.}

        \item \onslide<7->{The experiment is to measure the time between the
            first and second spikes in an experimental trial.} \onslide<9->{The
            sample space is the set $(0,\infty)$ of positive real numbers.}
            \onslide<9->{We can assign a probability of $1-\exp(-\lambda t)$ to
            the event that the second spike is fired less than $t$ seconds
            after the first spike.}

    \end{enumerate}
    \normalsize

    \note{

        - highlight that we discussed finite, countable and uncountable sample
        spaces.

        - Cantor (1845-1918): the set of all real numbers, and the set of real
        numbers between zero and one, are examples of infinite, and
        uncountable, sample spaces.

        - mention that the probability of a sample in an uncountable sample
        space is zero.

    }

\end{frame}

\begin{frame}
\frametitle{Axioms of probability theory}


    \begin{probAxiom}

        $P(A)\ge 0,\quad\forall A\in\mathcal{F}$

    \end{probAxiom}

    \begin{probAxiom}

        $P(\Omega)=1$

    \end{probAxiom}

    \begin{probAxiom}

        $P\left(\bigcup_{i=1}^\infty A_i\right)=\sum_{i=1}^\infty P(A_i)$ for every collection of pairwise disjoint events $A_1,A_2,\ldots$

    \end{probAxiom}


    \note{

        - explain what an infinite union means

        - For a finite or countable sample space it suffices that

            - $P(\omega)\ge 0$
            - $\sum_{\omega\in\Omega}P(\omega)=1$

    }

\end{frame}

\begin{frame}
    \frametitle{Experiment with equally likely outcomes}

    An experiment with equally likely outcomes is one with a finite number of
    outcomes $\omega_1,\ldots,\omega_N$, where all outcomes have the same
    probability (i.e., $P(\omega_i)=\frac{1}{N}$).

    \onslide<2->{

    \begin{claim}
        For any event $A$, $P(A)=\frac{N(A)}{N}$, where $N(A)$ is the number of
        outcomes in the set $A$.
    \end{claim}

    }

\end{frame}

\begin{frame}
    \frametitle{Example: equally likely outcomes}

    \begin{probExample}
        John, Pedro and Rosita each roll on fair die. How do we calculate the
        probability that the score of Rosita is equal to the sum of the scores
        of John and Pedro?
    \end{probExample}

    \note{

        - examples

        - uncountable sample space

    }

\end{frame}

\subsection{Some basic rules}

\setstretch{0.75}

\begin{frame}
    \frametitle{Some basic rules}

    \scriptsize
    \begin{probRule}
        For any finite number of mutually exclusive events $A_1,\ldots,A_N$,
        \begin{align*}
            P(A_1\cup A_2\cup\ldots\cup A_n) = P(A_1) + \ldots + P(A_N)
        \end{align*}
    \end{probRule}

    \begin{probRule}
        For any event A,
        \begin{align*}
            P(A) = 1 - P(A^c)
        \end{align*}
        where the event $A^c$ consists of all outcomes that are not in $A$.
    \end{probRule}

    \begin{probRule}
		Let $A,B$ be two events such that $A\subseteq B$. Then $P(A)\le P(B)$.
    \end{probRule}

    \begin{probRule}
        For any two events A and B,
        \begin{align*}
            P(A\cup B) = P(A) + P(B) - P(A\cap B)
        \end{align*}
    \end{probRule}

    \normalsize
\end{frame}

\setstretch{1.00}

\begin{frame}
    \frametitle{Rule 1}

	\footnotesize
	\begin{proof}
		Denote by $\emptyset$ the empty set. We will firt prove that
        $P(\emptyset)=0$. Take $A_i=\emptyset$ for $i=1, 2, \ldots$. Then
        $\emptyset=\bigcup_{i=1}^\infty A_i$. Next, by Axiom~3, $P(\emptyset)=\sum_{i=1}^\infty P(A_i)=\sum_{i=1}^\infty P(\emptyset)$. This implies that $P(\emptyset)=0$.

		Define $A_{N+j}=\emptyset$ for $j=1,2,\ldots$. Then
		\footnotesize
		\begin{align*}
            P(A_1\cup A_2\cup\ldots\cup A_N)&=P(A_1\cup A_2\cup\ldots\cup A_N\cup A_{N+1}\cup A_{N+2}\cup\ldots)\\
										    &=P(\bigcup_{i=1}^\infty A_i)=\sum_{i=1}^\infty P(A_i)\\
                                            &=\sum_{i=1}^N P(A_i)+\sum_{j=1}^\infty P(A_{N+j})=\sum_{i=1}^N P(A_i)
		\end{align*}
		\normalsize

	\end{proof}

	Notes:
	\begin{enumerate}
		\item the last equality in the second line holds by Axiom~1
		\item the last equality in the third line holds because $P(A_{N+j})=P(\emptyset)=0$.
	\end{enumerate}
    \normalsize

\end{frame}

\begin{frame}
    \frametitle{Proof of rule 2}

	\begin{proof}

		$\Omega=A\cup A^c$. $A$ and $A^c$ are disjoint. Then, by Rule~1, $P(\Omega)=P(A)+P(A^c)$. From Axiom~2, $P(\Omega)=1$. Thus, $1=P(A)+P(A^c)$.

	\end{proof}

\end{frame}

\begin{frame}
    \frametitle{Exercise}

    Prove rule 3.

\end{frame}

\begin{frame}
    \frametitle{Proof of rule 4}
    \tiny
	\begin{proof}
		\begin{align*}
            A\cup B&=\left(A\setminus B\right)\cup\left(B\setminus A\right)\cup\left(A\cap B\right)\\
            A&=\left(A\setminus B\right)\cup \left(A\cap B\right)\\
            B&=\left(B\setminus A\right)\cup \left(A\cap B\right)
		\end{align*}
        Since the sets in the right-hand-side of the above equations are
        pairwise disjoint, by rule 1, we obtain
		\begin{align*}
            P\left(A\cup B\right)&=P\left(A\setminus B\right)+P\left(B\setminus A\right)+P\left(A\cap B\right)\\
            P\left(A\right)&=P\left(A\setminus B\right)+P\left(A\cap B\right)\rightarrow P\left(A\setminus B\right)=P\left(A\right)-P\left(A\cap B\right)\\
            P\left(B\right)&=P\left(B\setminus A\right)+P\left(A\cap B\right)\rightarrow P\left(B\setminus A\right)=P\left(B\right)-P\left(A\cap B\right)
		\end{align*}
        Replazing the equations on the right of the second and third line in
        the equation on the first line the rule is proved.
		\begin{align*}
            P\left(A\cup B\right)&=P\left(A\right)-P\left(A\cap B\right)+P\left(B\right)-P\left(A\cap B\right)+P\left(A\cap B\right)\\
                                 &=P\left(A\right)+P\left(B\right)-P\left(A\cap B\right)
		\end{align*}
	\end{proof}
    \normalsize
\end{frame}

\begin{frame}
    \frametitle{Example 7.7: Chevalier de Mere to Blaise Pascal}

    The gambler Chevalier de Mere posed the following problem to the famous
    French mathematician Blaise Pascal in 1654. This problem marks the begining
    of probability theory.

    \begin{manualProbExample}{7.7}
        \begin{enumerate}[a]

            \item How many rolls of a fair die are required to have at least a
                50\% chance of rolling at least one six?

            \item How many rolls of two fair dice are required to have at
                least a 50\% chance of rolling at least one double six?

        \end{enumerate}
    \end{manualProbExample}

\end{frame}

\begin{frame}
    \frametitle{Analytical solution to example 7.7a}

	(a)
    \begin{itemize}[<+->]

		\item Let's fix the number of rolls $r$. The sample space is
$\Omega=\{(i_1,\ldots,i_r): 1\leq i_k\leq 6\}$, where $i_k$ is the up face of
the die on the kth roll. The outcomes in $\Omega$ are equiprobable.

        \item We want to calculate the probability of the event $A$=``at least
            one six shows up in the $r$ rolls.''. When you see the keyword ``at
            least one'' in an event, it is easier to calculate the probability
            of the complement $A^c$=``no six shows up in the $r$ rolls.''

        \item
            $P(A^c)=\frac{N(A^c)}{N}=\frac{5^r}{6^r}=\left(\frac{5}{6}\right)^r$.

        \item
            $\frac{1}{2}<P(A)=1-P(A^c)=1-\left(\frac{5}{6}\right)^r$
            iff
            $\left(\frac{5}{6}\right)^r<\frac{1}{2}$ iff
            $\log\left(\frac{5}{6}\right)^r<\log\frac{1}{2}$ iff
            $r\log\frac{5}{6}<\log\frac{1}{2}$ iff
            $r>\frac{\log\frac{1}{2}}{\log\frac{5}{6}}=3.8$
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Simulated solution to example 7.7a}

    Please see code \href{https://joacorapela.github.io/gcnuBridging2023/auto_examples/foundations/plot_example7_7a.html#sphx-glr-auto-examples-foundations-plot-example7-7a-py}{here}.

\end{frame}

\begin{frame}
    \frametitle{Exercise}

    Solve example 7.7b analytically and by simulation. Answer: you need at least 25 draws.

\end{frame}

\begin{comment}
\begin{frame}
    \frametitle{Analytical solution to example 7.7b}

	(b)
    \begin{itemize}[<+->]

		\item Let's fix the number of rolls $r$. The sample space is
$\Omega=\{(i^1_1,i^2_1,\ldots,i^1_r,i^2_r,): 1\leq i^j_k\leq 6\}$, where $i^j_k$ is the up face of
the jth die on the kth roll. The outcomes in $\Omega$ are equiprobable.

        \item We want to calculate the probability of the event $A$=``at least
            one pair of six shows up in the $r$ rolls.''. The complement of $A$ is
            $A^c$=``no pair of six shows up in the $r$ rolls.''

        \item
            $P(A^c)=\frac{N(A^c)}{N}=\frac{35^r}{36^r}=\frac{35^r}{36^r}=\left(\frac{35}{36}\right)^r$.

        \item
            $\frac{1}{2}<P(A)=1-P(A^c)=1-\left(\frac{35}{36}\right)^r$
            iff
            $\left(\frac{35}{36}\right)^r<\frac{1}{2}$ iff
            $\log\left(\frac{35}{3}\right)^r<\log\frac{1}{2}$ iff
            $r\log\frac{35}{36}<\log\frac{1}{2}$ iff
            $r>\frac{\log\frac{1}{2}}{\log\frac{35}{36}}=24.6$
    \end{itemize}
\end{frame}
\end{comment}

\begin{comment}
\begin{frame}
    \frametitle{Example:}

    - example 7.8 (rule 7-3, addition rule, easy)

\end{frame}
\end{comment}

\begin{frame}[fragile]
    \frametitle{Example 7.9: soccer teams in quarterfinal}

    \begin{manualProbExample}{7.9}
        The eight soccer teams which have reached the quarterfinals of the
        Championship League are formed by two teams from of the the countries
        England, Germany, Italy and Spain. The four matches to be played in the
        quarterfinal are determined by drawing lots.

        \begin{enumerate}[a]
            \item What is the probability that the two teams from the same
                country play against each other in each of the four matches?
            \item What is the probability that there is a match between the two
                teams from England or between the two teams from Germany?
        \end{enumerate}
    \end{manualProbExample}

\end{frame}

\begin{frame}
    \frametitle{Analytical solution to example 7.9: setup}
    \begin{itemize}[<+->]
        \item Let $T$ be the set of eight teams
            $T=\{E_1,E_2,G_1,G_2,I_1,I_2,S_1,S_2\}$
        \item A match is a set of two different elements from $T$ (i.e.,
            $\text{match}=\{T_i,T_j\},\text{match}\subset T,T_i\neq Tj$).
        \item A quarterfinal is a set of four different matches.
        \item $\Omega$ is the set of all possible quarterfinals (e.g.,
            $\{\{E1,G1\},\{I1,G2\},\{S2,G2\},\{I2,G1\}\}\in\Omega$).
        \item $\Omega$ contains equally-likely outcomes. Thus, for any event
            $A$, $P(A)=\frac{N(A)}{N}$.

        \item $N=\frac{\binom{8}{2}\binom{6}{2}\binom{4}{2}}{4!}$. There are
            $\binom{8}{2}$, $\binom{6}{2}$, $\binom{4}{2}$ and 1  ways of
            selecting the first, second, third and fourth matches,
            respectively. We divide by $4!$ because the order between matches
            does not matter.

    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Analytical solution to example 7.9a}
    \begin{itemize}[<+->]
        \item The event A=``\textit{two teams from the same country play against
            each other in each of the four matches}'' contains only one outcome
            (i.e., $A=\{\{E_1,E_2\},\{G_1,G_2\},\{I_1,I_2\},\{S_1,S_2\}\}$).

        \item
            $P(A)=\frac{N(A)}{N}=\frac{1}{\frac{\binom{8}{2}\binom{6}{2}\binom{4}{2}}{4!}}=0.009524$

    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Analytical solution to example 7.9b}
    \begin{itemize}[<+->]
        \scriptsize
        \item The event $B$=``\textit{there is a match between the two teams from
            England or between the two teams from Germany}'' is the union of
            the events $B_E$=``\textit{there is a match between the two teams
            from England}'' and $B_G$=``\textit{there is a match between the
            two teams from Germany}'' (i.e., $B=B_E\cup B_G$).

        \item Since $B_E$ and $B_G$ are not disjoint, we should use Rule~3 to
            compute $P(B_E\cup B_G)$ (i.e., $P(B_E\cup B_G)=P(B_E)+P(B_G)-P(B_E\cap B_G)$.

        \item $P(B_E)=P(B_G)$. Since the match $\{E_1,E_2\}$ is in all
            quarterfinals in $B_E$, to calculate $N(B_E)$ we need to computer
            the number of matches between six teams of three countries, as done
            in part (a). This gives
            $N(B_E)=\frac{\binom{6}{2}\binom{4}{2}}{3!}$. Then
            $P(B_E)=P(B_G)=\frac{\frac{\binom{6}{2}\binom{4}{2}}{3!}}{\frac{\binom{8}{2}\binom{6}{2}\binom{4}{2}}{4!}}=0.142857$.

        \item Since the match $\{E_1,E_2\}$ and $\{G_1,G_2\}$ are in all
            quarterfinals in $B_E\cap B_G$, as in part (a),
            $N(B_E\cap B_G)=\frac{\binom{4}{2}}{2!}$. Then $P(B_E\cap
            B_G)=\frac{\frac{\binom{4}{2}}{2!}}{\frac{\binom{8}{2}\binom{6}{2}\binom{4}{2}}{4!}}=0.028571$.

        \item Thus $P(B_E\cup B_G)=P(B_E)+P(B_G)-P(B_E\cap B_G)=2\times 0.142857-0.028571=0.257143$.
        \normalsize

    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Simulated solution to example 7.9a}

    Please see code \href{https://joacorapela.github.io/gcnuBridging2023/auto_examples/foundations/plot_example7_9a.html#sphx-glr-auto-examples-foundations-plot-example7-9a-py}{here}.

\end{frame}

\begin{frame}
    \frametitle{Exercise}

    Solve example 7.7b by simulation. Answer: you should obtain a solution
    close to the analytical one.

\end{frame}

\begin{comment}
\begin{frame}
    \frametitle{Example:}

    - example 7.10 (rule 7-1, birthday problem, used in example 8.6): uses counting tools (binomial coefficient)

\end{frame}
\end{comment}

\section{Conditional probability and Bayes}

\subsection{Conditional probability}

\begin{frame}
    \frametitle{Definition of conditional probability: motivation}

    \begin{itemize}[<+->]
        \item We are given a probability model $(\Omega,\mathcal{F},P)$.
        \item We are interested in the probability of event $A\in\mathcal{F}$.
            This model provides us the unconditioned probability of $A$, $P(A)$.

		\item Our colleague performs an experiment and tells us that event $B$
ocurred. How does the fact that $B$ occurred changes the probability of $A$?

            \begin{figure}
                \begin{tikzpicture}
                    % Place nodes
                    \node (pA) {$P(A)$};
                    \node [right=3cm of pA] (pAcB) {$P(A|B)$};
                    % Draw edges
                    % \path[->] (pA) edge node [below,anchor=center,pos=0.25] {B occurred} (pAcB);
                    \path[->] (pA) edge node [above] {B occurred} (pAcB);
                \end{tikzpicture}
            \end{figure}

		\item Definition 1: $P(\cdot|B)=P(\cdot\cap B)$, where $\cdot$ can be any event.

		\item Problem with Definition 1: we want $P(B|B)=1$, but from Definition 1, $P(B|B)=P(B\cap B)=P(B)\le 1$.

		\item Definition 2: $P(\cdot|B)=\frac{P(\cdot\cap B)}{P(B)}$.

			\begin{itemize}
				\item $P(B|B)=1\;\checkmark$
				\item $P(A)=P(A|\Omega)=\frac{P(A\cap\Omega)}{P(\Omega)}=\frac{P(A)}{1}=P(A)\;\checkmark$
			\end{itemize}

    \end{itemize}

	\note{
		- p. 256: good motivation of conditional probability in the cards example

		- Definition 8.1

		[- interpretation of condition probability with relative frequencies]

	}
\end{frame}

\begin{frame}
    \frametitle{Definition of conditional probability}

    \begin{probDef}[Conditional probability]

		For any to events $A$ and $B$, with $P(B)>0$, the conditional
probability of $A$ given $B$, $P(A|B)$, is defined as 

		\begin{align*}
			P(A|B)=\frac{P(A\cap B)}{P(B)}
		\end{align*}

	\end{probDef}

\end{frame}

\begin{frame}
    \frametitle{Example 8.1: conditional probability for two dice}

	\begin{manualProbExample}{8.1}
		Someone has rolled two dice. You know that one of the dice turned up a
face value of six. What is the probability that the other die turned up a six
as  well?
	\end{manualProbExample}

	\note{

		- Example 8.1 (first ask students their intuition, as the problem is counter intuitive)

		- do NOT present example 8.2 at this point, as it requires the concept of independence

	}
\end{frame}

\begin{frame}
    \frametitle{Analytical solution to example 8.1}

    \begin{itemize}[<+->]

		\item $\Omega=\{(i_1,i_2): 1\le i_1,i_2\le6\}$. $N=36$. Equally probable outcomes.

		\item $B$=``\textit{one die turned up a face value of six}''=
$\{(6,i),(j,6),(6,6):1\le i,j\le 5\}$. $N(B)=11$. $P(B)=\frac{N(B)}{N}=\frac{11}{36}$.

		\item $A$=``\textit{the other die turned up a face value of six}''

		\item $A\cap B$=``\textit{the two dice turned up a face value of six}''=
$\{(6,6)\}$. $N(A\cap B)=1$. $P(A\cap B)=\frac{N(A\cap B)}{N}=\frac{1}{36}$.

		\item Approach 1 [definition]: $P(A|B)=\frac{P(A\cap B)}{P(B)}=\frac{1/36}{11/36}=\frac{1}{11}$.

		\item Approach 2 [$B$ as $\Omega$]: $P(A|B)=\frac{N(A\cap B)}{N(B)}=\frac{1}{11}$.

    \end{itemize}

\end{frame}

\begin{frame}[fragile]
    \frametitle{Simulated solution to example 8.1}

    Please see code \href{https://joacorapela.github.io/gcnuBridging2023/auto_examples/foundations/plot_example8_1.html#sphx-glr-auto-examples-foundations-plot-example8-1-py}{here}.

\end{frame}

\begin{frame}
    \frametitle{Exercise}

    Someone has rolled two dice. You know that the first die turned up a
face value of six. What is the probability that the second die turned up a six
as  well?

\end{frame}

\subsection{Assigning probabilities by conditioning}

\begin{frame}
    \frametitle{Assigning probabilities by conditioning}

    \scriptsize
    \begin{probRule}

        For any $n\in\mathbb{N}$, $n\ge 2$, and any sequence of events $A_1,\ldots,A_n$,
            \begin{align*}
                P(A_1\cap\ldots\cap A_n) = P(A_n|A_{n-1}\cap\ldots\cap A_1)
                P(A_{n-1}|A_{n-2}\cap\ldots\cap A_1)\ldots P(A_1)
            \end{align*}
    \end{probRule}

\end{frame}

\begin{frame}
    \frametitle{Proof of rule 5}

    \tiny
    \begin{proof}
        By induction.

        \begin{description}
            \item[$P_2$:] $P(A_1\cap A_2)=P(A_2|A_1)P(A_1)$
            \item[$P_n\rightarrow P_{n+1}$:]
                \begin{align*}
                    P(A_1\cap A_2\cap\ldots\cap A_n\cap A_{n+1})=\;&P(B\cap A_{n+1})\\
                    =\;&P(A_{n+1}|B)P(B)\\
                    =\;&P(A_{n+1}|A_n\cap A_{n-1}\cap\ldots A_1)\\
                       &P(A_n\cap A_{n-1}\cap\ldots\cap A_1)\\
                    =\;&P(A_{n+1}|A_n\cap A_{n-1}\cap\ldots A_1)\\
                       &P(A_n|A_{n-1}\cap\ldots\cap A_1)\\
                       &P(A_{n-1}|A_{n-2}\cap\ldots\cap A_1)\ldots P(A_1)
                \end{align*}
        \end{description}
    \end{proof}
    Notes:
    \begin{enumerate}
        \item $P_2$ follows from the definition of conditional probability.
        \item in the first equality in $P_n\rightarrow P_{n+1}$ we defined the
            event $B=A_1\cap\ldots\cap A_n$.
        \item in the second equality in $P_n\rightarrow P_{n+1}$ we used the
            definition of conditional proability.
        \item in the third equality in $P_n\rightarrow P_{n+1}$ we replaced $B$
            by its definition.
        \item in the fourth equality in $P_n\rightarrow P_{n+1}$ we used the
            inductive hypothesis $P(A_1\cap\ldots\cap A_n) =
            P(A_n|A_{n-1}\cap\ldots\cap A_1)P(A_{n-1}|A_{n-2}\cap\ldots\cap
            A_1)\ldots P(A_1)$.
    \end{enumerate}
\end{frame}

\begin{frame}
    \frametitle{Example:}

    - redo Example 7.9 (solution following Rule 4)

\end{frame}

\begin{frame}
    \frametitle{Example:}

    - probability that it takes 10 or more cards before the first ace appears

\end{frame}

\subsection{Independent events}

\begin{frame}
    \frametitle{Independent events}

- motivation of independence definition with conditional probabilities

- Definition 8.2

\end{frame}

\begin{frame}
    \frametitle{Example}

- Example 8.5

\end{frame}

\begin{frame}
    \frametitle{Example}

- Example 8.6 (uses birthday problem, example 7.10)

\end{frame}

\subsection{Law of conditional probability}

\begin{frame}
    \frametitle{Law of conditional probability}

- example of die followed by coin tosses

    \begin{description}

        \item{Rule 5} law of conditional probability. Let $A$ be an event that
            can only occur if one of the mutually exclusive events
            $B_1,\ldots,B_n$ occurs. Then

            \begin{align*}
                P(A) = P(A|B_1) P(B_1) + \ldots + P(A|B_n) P(B_n)  
            \end{align*}

    \end{description}

\end{frame}

\begin{frame}
    \frametitle{Example}

- example 8.6: tour the France (difficult!)

\end{frame}

\subsection{Baye's rule in odds form}

\begin{frame}
    \frametitle{Bayes rule in odds form}

- true/false hypothesis

    \begin{description}

        \item{Rule 6} The posterior probability $P(H|E)$ satisfies

            \begin{align*}
                \frac{P(H|E)}{P(\bar{H}|E)} = \frac{P(H)}{P(\bar{H})}\frac{P(E|H)}{P(E|\bar{H})}
            \end{align*}

    \end{description}

- interpretation of rule 6

    - avoid need of P(E)

    - prior odds + likelihood ratio or Bayes factor

    - prior odds update with new evidence

    - sequential update (mention Bayesian linear regression)

\end{frame}

\begin{frame}
    \frametitle{Example}

- example 8.8

\end{frame}

\begin{frame}
    \frametitle{Example}

- example 8.11

    - add to the problem statement:

        - in 1992, 4936 women were murdered in the US, of which roughly 1430 were murdered by their (ex)husbands or boyfriends

        - 5\% of the married women in the US have at some point been physically abused by their husbands.

        - assume that a woman who has been murdered by some other than her husband had the same same chance of being abused by her husband as a randomly selected woman

        - Alan Dershowitz admitted that a substantial percentage of the husbands who murder their wives, previous to the murder, also physically abuse their wives. Given this statement, we assume that the proability that a husband physically abused his wife, given that he killed her, is 50 percent.

\end{frame}

\subsection{Bayesian inference -- discrete case}

\begin{frame}
    \frametitle{Bayesian inference -- discrete case}

- explain posterior sequential update

\end{frame}

\begin{frame}
    \frametitle{Example}

- example 8.13 (solve it analytically and by sampling)

\end{frame}

\begin{frame}
    \frametitle{References}

    \tiny{
        \bibliographystyle{apalike}
        \bibliography{probability}
    }
\end{frame}

\begin{comment}
\end{comment}

\end{document}

