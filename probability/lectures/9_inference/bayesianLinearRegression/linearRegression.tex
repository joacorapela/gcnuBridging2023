%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Beamer Presentation
% LaTeX Template
% Version 1.0 (10/11/12)
%
% This template has been downloaded from:
% http://www.LaTeXTemplates.com
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND THEMES
%----------------------------------------------------------------------------------------

\documentclass[11pt]{beamer}

\mode<presentation> {

% The Beamer class comes with a number of default slide themes
% which change the colors and layouts of slides. Below this is a list
% of all the themes, uncomment each in turn to see what they look like.

%\usetheme{default}
%\usetheme{AnnArbor}
%\usetheme{Antibes}
%\usetheme{Bergen}
%\usetheme{Berkeley}
%\usetheme{Berlin}
%\usetheme{Boadilla}
%\usetheme{CambridgeUS}
%\usetheme{Copenhagen}
%\usetheme{Darmstadt}
%\usetheme{Dresden}
%\usetheme{Frankfurt}
%\usetheme{Goettingen}
%\usetheme{Hannover}
%\usetheme{Ilmenau}
%\usetheme{JuanLesPins}
%\usetheme{Luebeck}
%\usetheme{Madrid}
%\usetheme{Malmoe}
%\usetheme{Marburg}
%\usetheme{Montpellier}
\usetheme{PaloAlto}
%\usetheme{Pittsburgh}
%\usetheme{Rochester}
%\usetheme{Singapore}
%\usetheme{Szeged}
%\usetheme{Warsaw}

% As well as themes, the Beamer class has a number of color themes
% for any slide theme. Uncomment each of these in turn to see how it
% changes the colors of your current slide theme.

%\usecolortheme{albatross}
%\usecolortheme{beaver}
%\usecolortheme{beetle}
%\usecolortheme{crane}
%\usecolortheme{dolphin}
%\usecolortheme{dove}
%\usecolortheme{fly}
%\usecolortheme{lily}
%\usecolortheme{orchid}
%\usecolortheme{rose}
%\usecolortheme{seagull}
%\usecolortheme{seahorse}
%\usecolortheme{whale}
%\usecolortheme{wolverine}

%\setbeamertemplate{footline} % To remove the footer line in all slides uncomment this line
%\setbeamertemplate{footline}[page number] % To replace the footer line in all slides with a simple slide count uncomment this line

%\setbeamertemplate{navigation symbols}{} % To remove the navigation symbols from the bottom of all slides uncomment this line
}

% remove title and author from left panel
 \makeatletter
  \setbeamertemplate{sidebar \beamer@sidebarside}%{sidebar theme}
  {
    \beamer@tempdim=\beamer@sidebarwidth%
    \advance\beamer@tempdim by -6pt%
    \insertverticalnavigation{\beamer@sidebarwidth}%
    \vfill
    \ifx\beamer@sidebarside\beamer@lefttext%
    \else%
      \usebeamercolor{normal text}%
      \llap{\usebeamertemplate***{navigation symbols}\hskip0.1cm}%
      \vskip2pt%
    \fi%
  }%
\makeatother
% done remove title and author from left panel 

\hypersetup{colorlinks,citecolor=cyan}
\usepackage{graphicx} % Allows including images
\usepackage{booktabs} % Allows the use of \toprule, \midrule and \bottomrule in tables
\usepackage{natbib}
\usepackage{apalike}
\usepackage{comment}
% \usepackage{enumitem}
% \setlist[itemize]{topsep=0pt,before=\leavevmode\vspace{-1.5em}}
% \setlist[description]{style=nextline}
\usepackage{amsthm}
\usepackage{media9}
% \usepackage{multimedia}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{tikz}
\tikzset{
     arrow/.style={-{Stealth[]}}
     }
\usetikzlibrary{positioning,arrows.meta}
\usetikzlibrary{shapes.geometric}

\setbeamertemplate{navigation symbols}{}%remove navigation symbols

\usepackage{setspace}

\newtheorem{probDef}{Definition}
\newtheorem*{probDef*}{Definition}
\newtheorem{claim}{Claim}
\newtheorem*{claim*}{Claim}
\newtheorem*{lemma*}{Lemma}
\newtheorem{probExample}{Example}
\newtheorem{probRule}{Rule}
\newtheorem{probAxiom}{Axiom}
\setbeamertemplate{theorems}[numbered]
\newtheorem{probExercise}{Exercise}

\newtheorem{manualprobRuleinner}{Rule}
\newenvironment{manualProbRule}[1]{%
  \renewcommand\themanualprobRuleinner{#1}%
  \manualprobRuleinner
}{\endmanualprobRuleinner}

\newtheorem{manualprobExampleinner}{Example}
\newenvironment{manualProbExample}[1]{%
  \renewcommand\themanualprobExampleinner{#1}%
  \manualprobExampleinner
}{\endmanualprobExampleinner}

\newcounter{saveenumi}
\newcommand{\seti}{\setcounter{saveenumi}{\value{enumi}}}
\newcommand{\conti}{\setcounter{enumi}{\value{saveenumi}}}
\newcommand{\keepi}{\addtocounter{saveenumi}{-1}\setcounter{enumi}{\value{saveenumi}}}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

%----------------------------------------------------------------------------------------
%	TITLE PAGE
%----------------------------------------------------------------------------------------

\title{Linear Regression}

\author{Joaqu\'{i}n Rapela} % Your name
\institute[GCNU, UCL] % Your institution as it will appear on the bottom of every slide, may be shorthand to save space
{
Gatsby Computational Neuroscience Unit\\University College London % Your institution for the title page
}
\date{\today} % Date, can be changed to a custom date

\AtBeginSection[]
{
	\begin{frame}<beamer>
    \frametitle{Contents}
		\tableofcontents[currentsection]
    \end{frame}
}
\AtBeginSubsection[]
{
    \begin{frame}{Outline}
        \tableofcontents[currentsection,currentsubsection]
    \end{frame}
}

\begin{document}

\begin{frame}
\titlepage % Print the title page as the first slide
\end{frame}

\begin{frame}
\frametitle{Contents} % Table of contents slide, comment this block out to remove it
\tableofcontents % Throughout your presentation, if you choose to use \section{} and \subsection{} commands, these will automatically be printed on this slide as an overview of your presentation
\end{frame}

\begin{frame}
\frametitle{Main reference} % Table of contents slide, comment this block out to remove it

    I will mainly follow chapters two \textit{Probability distributions} and
    three \textit{Linear models for regression} from
    \citet{bishop06}.

    \begin{center}
        \includegraphics[width=1.5in]{figures/bishop06Cover.png}
    \end{center}

\end{frame}

\begin{comment}
\begin{frame}
\frametitle{Main lecture goals}

    % \resizebox{5}{3}{%
    % \scalebox{0.7}{%
    \begin{figure}
        \begin{tikzpicture}
            % Place nodes
            \node[ellipse,align=center,anchor=north,fill=blue!20,draw=blue] (foundations) {Mathematical\\foundations\\of probability};
            \node[ellipse,align=center,anchor=north,fill=orange!20,draw=orange] [below left=of foundations] (proofs) {Proofs\\(beautiful)};
            \node[ellipse,align=center,anchor=north,fill=orange!20,draw=orange] [below=of foundations] (examples) {Examples\\(fun)};
            \node[ellipse,align=center,anchor=north,fill=orange!20,draw=orange] [below right=of foundations] (simulations) {Simulations\\(useful)};
            % Draw edges
            \path[-] (foundations) edge node [] {} (proofs);
            \path[-] (foundations) edge node [] {} (examples);
            \path[-] (foundations) edge node [] {} (simulations);
        \end{tikzpicture}
    \end{figure }

\end{frame}
\end{comment}

\section{The Gaussian distribution}

\begin{frame}
\frametitle{The Gaussian distribution}

    \begin{itemize}
        \item One-dimensional
        \begin{align*}
            \mathcal{N}(x|\mu,\sigma^2)=\frac{1}{(2\pi)^\frac{1}{2}(\sigma^2)^\frac{1}{2}}\exp\left\{-\frac{1}{2}\frac{(x-\mu)^2}{\sigma^2}\right\}
        \end{align*}
        \item D-dimensional
        \begin{align*}
            \mathcal{N}(\mathbf{x}|\boldsymbol{\mu},\Sigma)=\frac{1}{(2\pi)^{D/2}\Sigma^{\frac{1}{2}}}\exp\left\{-\frac{1}{2}(\mathbf{x}-\boldsymbol{\mu})^\intercal\Sigma^{-1}(\mathbf{x}-\boldsymbol{\mu})\right\}
        \end{align*}
    \end{itemize}

\end{frame}

\begin{frame}
    \frametitle{The Gaussian is the maximum entropy distribution
    \citep{coverAndThomas91}}

    \begin{probDef}[Differential entropy]
        The \textit{differential entropy} $h(X)$ of a continuous random
        variable X with a density $f(x)$ is defined as

        \begin{align*}
            h(X)=-\int_Sf(X)\log f(x)\ dx
        \end{align*}

        where $S$ is the support set of the random variable.
    \end{probDef}

    \begin{theorem}[The Gaussian is the maximum entropy distribution]
        Let the random vector $X\in\mathbb{R}^n$ have zero mean and covariance
        $K$. Then $h(X)\le\frac{1}{2}\log(2\pi e)^n|K|$, with equality if
        $X\sim\mathcal{N}(0,K)$.
    \end{theorem}

\end{frame}

\begin{frame}
    \frametitle{The central limit theorem \citep{papoulisAndPillai02}}

	\begin{theorem}[The central limit theorem]
		Given $n$ independent and identically distributed random vectors $\mathbf{X}_i$, with mean vector $\boldsymbol{\mu}=E\{\mathbf{X}_i\}$ and covariance matrix $\mathbf{\Sigma}$. Then

		\begin{align*}
			\sqrt{n}(\bar{\mathbf{X}}_n-\boldsymbol{\mu})\rightarrow\mathcal{N}(0,\Sigma)
		\end{align*}

		with convergence in distribution.
	\end{theorem}
\end{frame}

\begin{frame}
    \frametitle{Very useful properties of the Gaussian distribution \citep{bishop06}}

	\small
	\begin{theorem}[Marginals and conditionals of Gaussians are Gaussians]
		\label{thm:marginalOrConditionalOfGaussianIsGaussian}

		Given $\mathbf{x}=\left[\begin{array}{c}
									\mathbf{x}_a\\
									\mathbf{x}_b\\
								\end{array}\right]$ such that

		\begin{align*}
			p(\mathbf{x})&=\mathcal{N}\left(\mathbf{x}\left|
				\left[\begin{array}{c}
					      \boldsymbol{\mu}_a\\
						  \boldsymbol{\mu}_b\\
				   	  \end{array}\right],
				\left[\begin{array}{cc}
					      \Sigma_{aa} & \Sigma_{ab}\\
						  \Sigma_{ba} & \Sigma_{bb}\\
					  \end{array}\right]
			\right.\right)\\
			            &=\mathcal{N}\left(\mathbf{x}\left|
				\left[\begin{array}{c}
					      \boldsymbol{\mu}_a\\
						  \boldsymbol{\mu}_b\\
				   	  \end{array}\right],
				\left[\begin{array}{cc}
					      \Lambda_{aa} & \Lambda_{ab}\\
						  \Lambda_{ba} & \Lambda_{bb}\\
					  \end{array}\right]^{-1}
			\right.\right)
		\end{align*}
		Then
		\begin{align}
			p(\mathbf{x}_a|\mathbf{x}_b)&=\mathcal{N}\left(\mathbf{x}_a\left|\boldsymbol{\mu}_a-\Lambda_{aa}^{-1}\Lambda_{ab}(\mathbf{x}_b-\boldsymbol{\mu}_b),\Lambda_{aa}^{-1}\right.\right)\label{eq:gaussianCond1}\\
			                            &=\mathcal{N}\left(\mathbf{x}_a\left|\boldsymbol{\mu}_a+\Sigma_{ab}\Sigma_{bb}^{-1}(\mathbf{x}_b-\boldsymbol{\mu}_b),\Sigma_{aa}-\Sigma_{ab}\Sigma_{bb}^{-1}\Sigma_{ba}\right.\right)\label{eq:gaussianCond2}\\
            p(\mathbf{x}_b)&=\mathcal{N}\left(\mathbf{x}_b\left|\boldsymbol{\mu}_b,\Sigma_{bb}\right.\right)\label{eq:gaussianMarginal}
		\end{align}

	\end{theorem}
	\normalsize
\end{frame}

\begin{frame}
    \frametitle{Very useful properties of the Gaussian distribution \citep{bishop06}}

	\small
	\begin{theorem}[Marginals and conditionals of the linear Gaussian model]

		Given the linear Gaussian model

		\begin{align*}
			p(\mathbf{x})&=\mathcal{N}(\mathbf{x}|\boldsymbol{\mu},\Lambda^{-1})\\
			p(\mathbf{t}|\mathbf{x})&=\mathcal{N}(\mathbf{t}|A\boldsymbol{\mu}+\mathbf{b},L^{-1})
		\end{align*}

		Then

		\begin{align}
            p(\mathbf{t})&=\mathcal{N}(\mathbf{t}|A\boldsymbol{\mu}+\mathbf{b},L^{-1}+A\Lambda^{-1}A^\intercal)\label{eq:marginalLinearGaussianModel}\\
			p(\mathbf{x}|\mathbf{t})&=\mathcal{N}(\mathbf{x}|\Sigma\{A^\intercal L(\mathbf{t}-\mathbf{b})+\Sigma\boldsymbol{\mu}\},\Sigma)~\label{eq:conditionalLinearGaussianModel}
		\end{align}

		where

		\begin{align*}
			\Sigma=(\Lambda+A^\intercal LA)^{-1}
		\end{align*}

	\end{theorem}
	\normalsize
\end{frame}

\begin{frame}
    \frametitle{Very useful properties of the Gaussian distribution \citep{bishop06}}

	The conditional, $p(\mathbf{x}|\mathbf{t})$, of the linear Gaussian model is the fundamental result used in the derivation of

	\begin{enumerate}

		\item Bayesian linear regression~\citep{bishop06},

		\item Gaussian process regression~\citep{williamsAndRasmussen06},

		\item Gaussian process factor analysis~\citep{yuEtAl09},

		\item linear dynamical systems~\citep{durbinAndKoopman12}.

	\end{enumerate}

\end{frame}

\begin{frame}
    \frametitle{Proof: the conditional of a Gaussian is a Gaussian (Theorem~\ref{thm:marginalOrConditionalOfGaussianIsGaussian}, Eq.~\ref{eq:gaussianCond1})}

	\begin{claim}[Quadratic form of Gaussian log pdf]
		\label{claim:quadratricFormOfGaussianPDF}

		$p(\mathbf{x})$ is a Gaussian pdf with mean $\boldsymbol{\mu}$ and precision matrix $\Lambda$ if and only if $\int p(\mathbf{x}) d\mathbf{x}=1$ and

		\begin{align}
			\log p(\mathbf{x})=-\frac{1}{2}(\mathbf{x}^\intercal\Lambda\mathbf{x}-2\mathbf{x}^\intercal\Lambda\boldsymbol{\mu})+K\label{eq:gaussianQuadratic}
		\end{align}

		where $K$ is a constant that does not depend on $\mathbf{x}$.

	\end{claim}

\end{frame}

\begin{frame}
    \frametitle{Proof: the conditional of a Gaussian is a Gaussian (Theorem~\ref{thm:marginalOrConditionalOfGaussianIsGaussian}, Eq.~\ref{eq:gaussianCond1})}

	\begin{proof}[Proof of Claim~\ref{claim:quadratricFormOfGaussianPDF}]

		\scriptsize
		\begin{description}
			\item[$\rightarrow)$]

				\begin{align*}
					p(\mathbf{x})&=\frac{1}{(2\pi)^{D/2}\Lambda^{-\frac{1}{2}}}\exp\left\{-\frac{1}{2}(\mathbf{x}-\boldsymbol{\mu})^\intercal\Lambda(\mathbf{x}-\boldsymbol{\mu})\right\}\\
					\log p(\mathbf{x})&=-\frac{1}{2}(\mathbf{x}-\boldsymbol{\mu})^\intercal\Lambda(\mathbf{x}-\boldsymbol{\mu})-\log ((2\pi)^{D/2}\Lambda^{-\frac{1}{2}})\\
					                  &=-\frac{1}{2}(\mathbf{x}^\intercal\Lambda\mathbf{x}-2\mathbf{x}^\intercal\Lambda\boldsymbol{\mu})-\frac{1}{2}\boldsymbol{\mu}^\intercal\Lambda\boldsymbol{\mu}-\log ((2\pi)^{D/2}\Lambda^{-\frac{1}{2}})\\
					                  &=-\frac{1}{2}(\mathbf{x}^\intercal\Lambda\mathbf{x}-2\mathbf{x}^\intercal\Lambda\boldsymbol{\mu})+K
				\end{align*}
				with $K=-\frac{1}{2}\boldsymbol{\mu}^\intercal\Lambda\boldsymbol{\mu}-\log ((2\pi)^{D/2}\Lambda^{-\frac{1}{2}})$.
				\phantom\qedhere
		\end{description}
		\normalsize
	\end{proof}
\end{frame}

\begin{frame}
    \frametitle{Proof: the conditional of a Gaussian is a Gaussian (Theorem~\ref{thm:marginalOrConditionalOfGaussianIsGaussian}, Eq.~\ref{eq:gaussianCond1})}

	\begin{proof}[Proof of Claim~\ref{claim:quadratricFormOfGaussianPDF}]

		\scriptsize
		\begin{description}
			\item[$\leftarrow)$]

				\begin{align}
					\log p(\mathbf{x})=&-\frac{1}{2}(\mathbf{x}^\intercal\Lambda\mathbf{x}-2\mathbf{x}^\intercal\Lambda\boldsymbol{\mu})+K\nonumber\\
					\log p(\mathbf{x})=&-\frac{1}{2}(\mathbf{x}^\intercal\Lambda\mathbf{x}-2\mathbf{x}^\intercal\Lambda\boldsymbol{\mu})-\frac{1}{2}\boldsymbol{\mu}^\intercal\Lambda\boldsymbol{\mu}-\log ((2\pi)^{D/2}\Lambda^{-\frac{1}{2}})\nonumber\\
					                   &+K+\frac{1}{2}\boldsymbol{\mu}^\intercal\Lambda\boldsymbol{\mu}+\log ((2\pi)^{D/2}\Lambda^{-\frac{1}{2}})\nonumber\\
					                  =&-\frac{1}{2}(\mathbf{x}-\boldsymbol{\mu})^\intercal\Lambda(\mathbf{x}-\boldsymbol{\mu})-\log ((2\pi)^{D/2}\Lambda^{-\frac{1}{2}})\nonumber\\
					                   &+K+\frac{1}{2}\boldsymbol{\mu}^\intercal\Lambda\boldsymbol{\mu}+\log ((2\pi)^{D/2}\Lambda^{-\frac{1}{2}})\nonumber\\
					                  =&\log N(\mathbf{x}|\boldsymbol{\mu},\Lambda)+K+\frac{1}{2}\boldsymbol{\mu}^\intercal\Lambda\boldsymbol{\mu}+\log ((2\pi)^{D/2}\Lambda^{-\frac{1}{2}})\nonumber\\
					     p(\mathbf{x})=&N(\mathbf{x}|\boldsymbol{\mu},\Lambda)\exp\left(K+\frac{1}{2}\boldsymbol{\mu}^\intercal\Lambda\boldsymbol{\mu}+\log ((2\pi)^{D/2}\Lambda^{-\frac{1}{2}})\right)\label{eq:quadratricFormOfGaussianPDF_almostFinal}
				\end{align}
				\phantom\qedhere
		\end{description}
		\normalsize
	\end{proof}
\end{frame}

\begin{frame}
    \frametitle{Proof: the conditional of a Gaussian is a Gaussian (Theorem~\ref{thm:marginalOrConditionalOfGaussianIsGaussian}, Eq.~\ref{eq:gaussianCond1})}

	\begin{proof}[Proof of Claim~\ref{claim:quadratricFormOfGaussianPDF}]

		\scriptsize
		\begin{description}
			\item[$\leftarrow)$ cont]
				\begin{align*}
					1&=\int p(\mathbf{x})d\mathbf{x}\\
					 &=\int N(\mathbf{x}|\boldsymbol{\mu},\Lambda)\exp\left(K+\frac{1}{2}\boldsymbol{\mu}^\intercal\Lambda\boldsymbol{\mu}+\log ((2\pi)^{D/2}\Lambda^{-\frac{1}{2}})\right)d\mathbf{x}\\
					 &=\exp\left(K+\frac{1}{2}\boldsymbol{\mu}^\intercal\Lambda\boldsymbol{\mu}+\log ((2\pi)^{D/2}\Lambda^{-\frac{1}{2}})\right)\int N(\mathbf{x}|\boldsymbol{\mu},\Lambda)d\mathbf{x}\\
					 &=\exp\left(K+\frac{1}{2}\boldsymbol{\mu}^\intercal\Lambda\boldsymbol{\mu}+\log ((2\pi)^{D/2}\Lambda^{-\frac{1}{2}})\right)
				\end{align*}
				From Eq.~\ref{eq:quadratricFormOfGaussianPDF_almostFinal} then $p(\mathbf{x})=N(\mathbf{x}|\boldsymbol{\mu},\Lambda)$.
		\end{description}
		\normalsize
	\end{proof}
\end{frame}

\begin{frame}
    \frametitle{Proof: the conditional of a Gaussian is a Gaussian (Theorem~\ref{thm:marginalOrConditionalOfGaussianIsGaussian}, Eq.~\ref{eq:gaussianCond1})}

	\begin{proof}[Proof of Theorem~\ref{thm:marginalOrConditionalOfGaussianIsGaussian}, Eq.~\ref{eq:gaussianCond1}]

		\scriptsize
		\begin{align*}
			p(\mathbf{x}_a|\mathbf{x}_b)&=\frac{p(\mathbf{x}_a,\mathbf{x}_b)}{p(\mathbf{x}_b)}=\frac{p(\mathbf{x})}{p(\mathbf{x}_b)}\\
			\log p(\mathbf{x}_a|\mathbf{x}_b)&=\log p(\mathbf{x})-\log p(\mathbf{x}_b)=\log p(\mathbf{x})+K
		\end{align*}

		Therefore, the terms of $\log p(\mathbf{x}_a|\mathbf{x}_b)$ that depend on $\mathbf{x}_a$ are those of $\log p(\mathbf{x})$.

		Steps for the proof:

		\begin{enumerate}
			\item isolate the terms of $\log p(\mathbf{x})$ that depend on $\mathbf{x}_a$,
			\item notice that these term has the quadratic form of Claim~\ref{claim:quadratricFormOfGaussianPDF}, therefore $p(\mathbf{x}_a|\mathbf{x}_b)$ is Gaussian,
			\item identify $\boldsymbol{\mu}$ and $\Lambda$ in this quadratic form.
		\end{enumerate}

		\phantom\qedhere
		\normalsize
	\end{proof}
\end{frame}

\begin{frame}
    \frametitle{Proof: the conditional of a Gaussian is a Gaussian (Theorem~\ref{thm:marginalOrConditionalOfGaussianIsGaussian}, Eq.~\ref{eq:gaussianCond1})}

	\begin{proof}[Proof of Theorem~\ref{thm:marginalOrConditionalOfGaussianIsGaussian}, Eq.~\ref{eq:gaussianCond1}]

		\scriptsize
		\begin{align*}
			p(\mathbf{x})&=\frac{1}{(2\pi)^{D/2}|\Lambda|^{1/2}}\exp\left(-\frac{1}{2}(\mathbf{x}-\boldsymbol{\mu})^\intercal\Lambda(\mathbf{x}-\boldsymbol{\mu})\right)\\
			\log p(\mathbf{x})=&-\frac{1}{2}(\mathbf{x}-\boldsymbol{\mu})^\intercal\Lambda(\mathbf{x}-\boldsymbol{\mu})+K_1\\
			                  =&-\frac{1}{2}[(\mathbf{x}_a-\boldsymbol{\mu}_a)^\intercal,(\mathbf{x}_b-\boldsymbol{\mu}_b)^\intercal]\left[\begin{array}{cc}
							                                                                                                                   \Lambda_{aa} & \Lambda_{ab}\\
							                                                                                                                   \Lambda_{ba} & \Lambda_{bb}
																																	       \end{array}\right]
																																	 \left[\begin{array}{c}
																																	           \mathbf{x}_a-\boldsymbol{\mu}_a\\
																																	           \mathbf{x}_b-\boldsymbol{\mu}_b\\
																																			\end{array}\right]
																																	 +K_1\\
			                  =&-\frac{1}{2}\left\{(\mathbf{x}_a-\boldsymbol{\mu}_a)^\intercal\Lambda_{aa}(\mathbf{x}_a-\boldsymbol{\mu}_a)+2(\mathbf{x}_a-\boldsymbol{\mu}_a)^\intercal\Lambda_{ab}(\mathbf{x}_b-\boldsymbol{\mu}_b)\right.\\
							   &\left.+(\mathbf{x}_b-\boldsymbol{\mu}_b)^\intercal\Lambda_{bb}(\mathbf{x}_b-\boldsymbol{\mu}_b)\right\}+K_1\\
			                  =&-\frac{1}{2}\left\{\mathbf{x}_a^\intercal\Lambda_{aa}\mathbf{x}_a-2\mathbf{x}_a^\intercal(\Lambda_{aa}\boldsymbol{\mu}_a-\Lambda_{ab}(\mathbf{x}_b-\boldsymbol{\mu}_b))\right\}+K_2\\
			                  =&-\frac{1}{2}\left\{\mathbf{x}_a^\intercal\Lambda_{aa}\mathbf{x}_a-2\mathbf{x}_a^\intercal\Lambda_{aa}(\boldsymbol{\mu}_a-\Lambda_{aa}^{-1}\Lambda_{ab}(\mathbf{x}_b-\boldsymbol{\mu}_b))\right\}+K_2
		\end{align*}
		Comparing the last equation with Eq.~\ref{eq:gaussianQuadratic} we see that $\Lambda=\Lambda_{aa}$, $\boldsymbol{\mu}=\boldsymbol{\mu}_a-\Lambda_{aa}^{-1}\Lambda_{ab}(\mathbf{x}_b-\boldsymbol{\mu}_b)$ and conclude that $p(\mathbf{x}_a|\mathbf{x}_b)=\mathcal{N}(\mathbf{x}_a|\boldsymbol{\mu}_a-\Lambda_{aa}^{-1}\Lambda_{ab}(\mathbf{x}_b-\boldsymbol{\mu}_b),\Lambda_{aa})$
		\normalsize
	\end{proof}
\end{frame}

\begin{frame}
    \frametitle{Proof: the conditional of a Gaussian is a Gaussian (Theorem~\ref{thm:marginalOrConditionalOfGaussianIsGaussian}, Eq.~\ref{eq:gaussianCond2})}

	\begin{claim}[Inverse of a partitioned matrix]
		\begin{align}
			\left(\begin{array}{cc}
				      A & B\\
				      C & D
				  \end{array}^{-1}\right)=
			\left(\begin{array}{cc}
				      M         & -MBD^{-1}\\
					  -D^{-1}CM & D^{-1} + D^{-1}CMBD^{-1}
				  \end{array}\right)\label{eq:inversePartitionedMatrix}
		\end{align}
		where
		\begin{align*}
			M = (A - BD^{-1}C)^{-1}
		\end{align*}
	\end{claim}
	\begin{proof}
		\scriptsize
		Exercise. Hint: verify that the multiplication of the inverse of the matrix in the right hand side of Eq.~\ref{eq:inversePartitionedMatrix} with the matrix in the left hand side of the same equation is the identity matrix.
		\phantom\qedhere
		\normalsize
	\end{proof}
\end{frame}

\begin{frame}
    \frametitle{Proof: the conditional of a Gaussian is a Gaussian (Theorem~\ref{thm:marginalOrConditionalOfGaussianIsGaussian}, Eq.~\ref{eq:gaussianCond2})}

	\begin{proof}[Proof of Theorem~\ref{thm:marginalOrConditionalOfGaussianIsGaussian}, Eq.~\ref{eq:gaussianCond2}]
		\scriptsize
		Using the definition
		\begin{align*}
			\left(\begin{array}{cc}
				      \Sigma_{aa} & \Sigma_{ab}\\
				      \Sigma_{ba} & \Sigma_{bb}
                  \end{array}\right)^{-1}=
			\left(\begin{array}{cc}
				      \Lambda_{aa} & \Lambda_{ab}\\
				      \Lambda_{ba} & \Lambda_{bb}
                  \end{array}\right)
		\end{align*}
		and using Eq.~\ref{eq:inversePartitionedMatrix}, we obtain
		\begin{align*}
			\Lambda_{aa}&=(\Sigma_{aa}-\Sigma_{ab}\Sigma_{bb}^{-1}\Sigma_{ba})^{-1}\\
			\Lambda_{ab}&=-(\Sigma_{aa}-\Sigma_{ab}\Sigma_{bb}^{-1}\Sigma_{ba})^{-1}\Sigma_{ab}\Sigma_{bb}^{-1}
		\end{align*}
		Replacing the above equations in Eq.~\ref{eq:gaussianCond1} we obtain Eq.~\ref{eq:gaussianCond2}.
		\normalsize
	\end{proof}

\end{frame}

\section{Linear models for regression}

\begin{frame}
    \frametitle{Linear regression example}

	\begin{center}
		\includegraphics[width=2.2in]{figures/visVesIntegration.png}
	\end{center}
	\hfill\href{https://www.biorxiv.org/content/10.1101/2021.01.22.427789v4.abstract}{Keshavarzi et al., 2021}
	\begin{columns}
		\onslide<2->{
		\begin{column}{0.5\textwidth}
			\begin{center}
				\includegraphics[width=1.5in]{figures/spikeRateVsabsSpeedV1VisVes.png}
			\end{center}
		\end{column}
		}
		\onslide<3->{
		\begin{column}{0.5\textwidth}
			\textcolor{blue}{Is there a linear relation between the speed of rotation and the firing rate of visual cells?}
		\end{column}
		}
	\end{columns}
\end{frame}

\begin{frame}
    \frametitle{Estimating nonlinear receptive fields from natural images}

    \href{https://jov.arvojournals.org/article.aspx?articleid=2192869}{Rapela et al., 2006}.

\end{frame}

\begin{frame}
    \frametitle{Linear regression model}

	\scriptsize
	\begin{description}
		\item[simple linear regression model]
			\begin{align*}
                y(x_i, \mathbf{w})&=w_0+w_1x_i
				                   =\raisebox{0.60em}{$[1,x_i]$}
									\left[\begin{array}{c}
								        w_0\\
								        w_1
								    \end{array}\right]
				                   =\raisebox{0.60em}{$[\phi_0(x_i),\phi_1(x_i)]$}
									\left[\begin{array}{c}
								        w_0\\
								        w_1
								    \end{array}\right]\\
                                  & =
									\boldsymbol{\phi}(x_i)^\intercal\mathbf{w}
			\end{align*}
		\item[polynomial regression model]
			\begin{align*}
				y(x_i, \mathbf{w})&=w_0+w_1x_i+w_2x_i^2+w_3x_i^3
				                   =\raisebox{1.80em}{$[1,x_i,x_i^2,x_i^3]$}
									\left[\begin{array}{c}
								        w_0\\
								        w_1\\
								        w_2\\
								        w_3
								    \end{array}\right]\\
				                  &=\raisebox{1.80em}{$[\phi_0(x_i),\phi_1(x_i),\phi_2(x_i),\phi_3(x_i)]$}
									\left[\begin{array}{c}
								        w_0\\
								        w_1\\
								        w_2\\
								        w_3
								    \end{array}\right]=
									\boldsymbol{\phi}(x_i)^\intercal\mathbf{w}
			\end{align*}
		\item[basis functions linear regression model]
			\begin{align*}
				y(x_i, \mathbf{w})&=\boldsymbol{\phi}(x_i)^\intercal\mathbf{w}=\sum_{j=1}^Mw_j\phi_j(x_i)
			\end{align*}
	\end{description}
	\normalsize
\end{frame}

\begin{frame}
    \frametitle{Linear regression model}

    \scriptsize
    \begin{align*}
        \mathbf{y}(\mathbf{x},\mathbf{w})&=
            \left[\begin{array}{c}
                      y(x_1,\mathbf{w})\\
                      y(x_2,\mathbf{w})\\
                      \ldots\\
                      y(x_N,\mathbf{w})
                  \end{array}\right]=
            \left[\begin{array}{cccc}
                      \phi_1(x_1)&\phi_2(x_1)&\ldots&\phi_M(x_1)\\
                      \phi_1(x_2)&\phi_2(x_2)&\ldots&\phi_M(x_2)\\
                      \vdots     &\vdots     &\ldots&\vdots\\
                      \phi_1(x_N)&\phi_2(x_N)&\ldots&\phi_M(x_N)
            \end{array}\right]\left[\begin{array}{c}
                                        w_1\\
                                        w_2\\
                                        \vdots\\
                                        w_M
                                    \end{array}\right]\\
                                         &=\boldsymbol{\Phi}\mathbf{w}
    \end{align*}

    where
    $\mathbf{y}(\mathbf{x},\mathbf{w})\in\mathbb{R}^N,\boldsymbol{\Phi}\in\mathbb{R}^{N\times M},\mathbf{w}\in\mathbb{R}^M$.
    \normalsize
\end{frame}

\begin{frame}
    \frametitle{Basis functions for regression}

		\begin{center}
			\includegraphics[width=3.5in]{figures/basisFunctions.png}
		\end{center}
        \hfill\scriptsize\citet{bishop06}

        \begin{description}
            \item[polynomial] $\phi_i(x)=x^i$
            \item[Gaussian] $\phi_i(x)=\exp(-\frac{(x-\mu_i)^2}{2\sigma^2})$
            \item[sigmoidal]
                $\phi_i(x)=\frac{1}{1+\exp(-\frac{x-\mu_i}{\sigma^2})}$
        \end{description}
\end{frame}

\begin{frame}
    \frametitle{Example dataset}

    \begin{center}
        \includegraphics[width=4in]{figures/exampleDataset.png}
    \end{center}
    \hfill\scriptsize\citet{bishop06}

\end{frame}

\subsection{Least-squares regression}

\begin{frame}
    \frametitle{Least-squares estimation of model parameters
    \citep{trefethenAndBau97}}

    \scriptsize
    \begin{probDef}[Least-squares problem]
        Given $\boldsymbol{\Phi}\in\mathbb{R}^{N\times M},N\ge
        M,\mathbf{t}\in\mathbb{R}^N$, find $\mathbf{w}\in\mathbb{R}^M$ such
        that $E_{LS}(\mathbf{w})=||\mathbf{t}-\boldsymbol{\Phi}\boldsymbol{w}||_2$ is minimised.
    \end{probDef}
    \begin{theorem}[Least-squares solution]
        Let $\boldsymbol{\Phi}\in\mathbb{R}^{N\times M} (N\ge M)$ and
        $\mathbf{t}\in\mathbb{R}^N$ be given. A vector
        $\mathbf{w}\in\mathbb{R}^M$ minimises
        $||\mathbf{r}||_2=||\mathbf{t}-\boldsymbol{\Phi}\mathbf{w}||_2$, thereby solving the
        least-squares problem, if and only if
        $\mathbf{r}\perp\text{range}(\boldsymbol{\Phi})$, that is,
        $\boldsymbol{\Phi}^\intercal\mathbf{r}=0$,
        or equivalently,
        $\boldsymbol{\Phi}^\intercal\boldsymbol{\Phi}\mathbf{w}=\boldsymbol\Phi^\intercal\mathbf{t}$,
        or again equivalently,
        $P\mathbf{t}=\boldsymbol{\Phi}\mathbf{w}$.
    \end{theorem}
	\begin{center}
		\includegraphics[width=4in]{figures/leastSquares.png}
	\end{center}
    \hfill\scriptsize\citet{bishop06}
    \normalsize

    \note{
    Given a set of $N$ observations, $\mathbf{t}$, $N>M$, we want to find model
    parameters $\mathbf{w}$ such that the model outputs,
    $\mathbf{t}(\mathbf{x},\mathbf{w})$ equal the observations. This is
    generally impossible, because the degrees of freedom of the observations,
    $N$, is generally larger than the degrees of freedom of the model
    $\mathbf{t}(\mathbf{x},\mathbf{w})$, $M$. We instead solve the following
    least-squares problem.
    }
\end{frame}

\begin{frame}[fragile]
    \frametitle{Instruction to run notebooks in Google Colab}

    \begin{enumerate}
        \item open a notebook from
            \href{https://github.com/joacorapela/gcnuBridging2023/tree/master/docs/sphinx/build/html/notebooks/auto_examples/bayesianLinearRegression}{here}
        \item replace \textbf{github.com} by \textbf{githubtocolab.com} in the
            URL
        \item insert a cell at the beginning of the notebook with the following
            content
        \seti
    \end{enumerate}

    \tiny
    \begin{verbatim}
       !git clone https://github.com/joacorapela/gcnuBridging2023.git
       %cd gcnuBridging2023
       !pip install -e .
    \end{verbatim}
    \normalsize

    \begin{enumerate}
        \conti
        \item from the menu \textbf{Runtime} select \textbf{Run all}.
    \end{enumerate}
\end{frame}

\begin{frame}
    \frametitle{Code for least-squares estimation of model parameters}

    \begin{itemize}
        \item \href{https://joacorapela.github.io/gcnuBridging2023/auto\_examples/bayesianLinearRegression/plotOverfittingLeastSquares.html\#sphx-glr-auto-examples-bayesianlinearregression-plotoverfittingleastsquares-py}{overfitting}
        \item \href{https://joacorapela.github.io/gcnuBridging2023/auto\_examples/bayesianLinearRegression/plotCrossValidationLeastSquares.html\#sphx-glr-auto-examples-bayesianlinearregression-plotcrossvalidationleastsquares-py}{cross validation}
        \item \href{https://joacorapela.github.io/gcnuBridging2023/auto\_examples/bayesianLinearRegression/plotLackOfOverfittingInLeastSquaresForLargerDatasetSize.html\#sphx-glr-auto-examples-bayesianlinearregression-plotlackofoverfittinginleastsquaresforlargerdatasetsize-py}{larger datasets allow more complex models}
    \end{itemize}

\end{frame}

\begin{frame}
    \frametitle{Regularised least-squares estimation of model parameters}

    To cope with the overfitting of least squares, we can add to the least
    squares optimisation criterion a term that enforces coefficients to be
    zero. The regularised least-squares optimisation criterion becomes:

    \begin{align*}
        E_{RLS}(\mathbf{w})=||\mathbf{t}-\boldsymbol{\Phi}\mathbf{w}||_2^2+\lambda||\mathbf{w}||_2^2
    \end{align*}

    where $\lambda$ is the regularisation parameter that weights the strength
    of the regularisation.
\end{frame}

\begin{frame}
    \frametitle{Regularised least-squares estimation of model parameters}
	\scriptsize
	\begin{claim}[Regularized least-squares estimate]
		\begin{align*}
			\mathbf{w}_{RLS}=\argmin_{\mathbf{w}}E_{RLS}(\mathbf{w})=\argmin_{\mathbf{w}}||\mathbf{t}-\boldsymbol{\Phi}\mathbf{w}||_2^2+\lambda||\mathbf{w}||_2^2=(\lambda\mathbf{I}+\boldsymbol{\Phi}^\intercal\boldsymbol{\Phi})^{-1}\boldsymbol{\Phi}^\intercal\mathbf{t}
		\end{align*}
	\end{claim}
	\tiny
	\begin{proof}
		Since $E_{RLS}(\mathbf{w})$ is a polynomial of order two on the elements of $\mathbf{w}$ (i.e., a quadratic form), we can use the \emph{Completing the Squares} technique below to find its minimum.
		\begin{align}
			\boldsymbol{\mu}&=\argmax_{\mathbf{w}}\mathcal{N}(\mathbf{w}|\boldsymbol{\mu},\Sigma)=\argmax_{\mathbf{w}}\log\mathcal{N}(\mathbf{w}|\boldsymbol{\mu},\Sigma)\nonumber\\
                            &=\argmax_{\mathbf{w}}\{K-\frac{1}{2}(-2\boldsymbol{\mu}^\intercal\Sigma^{-1}\mathbf{w}+\mathbf{w}\Sigma^{-1}\mathbf{w})\}\label{eq:completingTheSquaresStep2}\\
                            &=\argmin_{\mathbf{w}}\{-K+\frac{1}{2}(-2\boldsymbol{\mu}^\intercal\Sigma^{-1}\mathbf{w}+\mathbf{w}\Sigma^{-1}\mathbf{w})\}\nonumber\\
                            &=\argmin_{\mathbf{w}}\{K_1-2\boldsymbol{\mu}^\intercal\Sigma^{-1}\mathbf{w}+\mathbf{w}\Sigma^{-1}\mathbf{w}\}\label{eq:completingTheSquares}
		\end{align}

		Note: Eq.~\ref{eq:completingTheSquaresStep2} uses Eq.~\ref{eq:gaussianQuadratic}.

		To find the minimum of a quadratic form, we write it in the form of the
terms inside the curly brackets of Eq.~\ref{eq:completingTheSquares}, and the
term corresponding to $\boldsymbol{\mu}$ will be the minimum.

		\phantom\qedhere
	\end{proof}
	\normalsize
\end{frame}

\begin{frame}
    \frametitle{Regularised least-squares estimation of model parameters}
	\tiny
		\begin{proof}
			Let's write $E_{RLS}$ in the form of the terms inside the curly brackets of Eq.~\ref{eq:completingTheSquares}.

		\begin{align*}
			E_{RLS}&=||\mathbf{t}-\boldsymbol{\Phi}\mathbf{w}||_2^2+\lambda||\mathbf{w}||_2^2=(\mathbf{t}-\boldsymbol{\Phi}\mathbf{w})^\intercal(\mathbf{t}-\boldsymbol{\Phi}\mathbf{w})+\lambda\mathbf{w}^\intercal\mathbf{w}\\
                   &=\mathbf{t}^\intercal\mathbf{t}-2\mathbf{t}^\intercal\boldsymbol{\Phi}\mathbf{w}+\mathbf{w}^\intercal\boldsymbol{\Phi}^\intercal\boldsymbol{\Phi}\mathbf{w}+\lambda\mathbf{w}^\intercal\mathbf{w}\\
                   &=\mathbf{t}^\intercal\mathbf{t}-2\mathbf{t}^\intercal\boldsymbol{\Phi}\mathbf{w}+\mathbf{w}^\intercal(\boldsymbol{\Phi}^\intercal\boldsymbol{\Phi}^\intercal+\lambda\mathbf{I}_M)\mathbf{w}
		\end{align*}
		Calling
		\begin{align*}
			\Sigma^{-1}&=\boldsymbol{\Phi}^\intercal\boldsymbol{\Phi}^\intercal+\lambda\mathbf{I}_M\\
			\boldsymbol{\mu}^\intercal\Sigma^{-1}&=\mathbf{t}^\intercal\boldsymbol{\Phi}\;\text{or}\;\boldsymbol{\mu}^\intercal=\mathbf{t}^\intercal\boldsymbol{\Phi}\Sigma\;\text{or}\;\boldsymbol{\mu}=\Sigma\boldsymbol{\Phi}^\intercal\mathbf{t}=\left(\boldsymbol{\Phi}^\intercal\boldsymbol{\Phi}^\intercal+\lambda\mathbf{I}_M\right)^{-1}\boldsymbol{\Phi}^\intercal\mathbf{t}
		\end{align*}
		we can express
		\begin{align*}
			E_{RLS}=K+2\boldsymbol{\mu}^\intercal\Sigma^{-1}\mathbf{w}+\mathbf{w}\Sigma^{-1}\mathbf{w}
		\end{align*}
		Then
		\begin{align*}
			 \mathbf{w}_{RLS}=\argmin_{\mathbf{w}}E_{RLS}(\mathbf{w})=\boldsymbol{\mu}=\left(\boldsymbol{\Phi}^\intercal\boldsymbol{\Phi}^\intercal+\lambda\mathbf{I}_M\right)^{-1}\boldsymbol{\Phi}^\intercal\mathbf{t} 
		\end{align*}
		\end{proof}
	\normalsize
\end{frame}

\begin{frame}
    \frametitle{Code for regularised least-squares estimation of model parameters}
    \begin{itemize}
        \item \href{file:///nfs/ghome/live/rapela/dev/teaching/gcnuBridging2023/repo/docs/sphinx/build/html/auto\_examples/bayesianLinearRegression/plotRegularizedLeastSquares.html\#sphx-glr-auto-examples-bayesianlinearregression-plotregularizedleastsquares-py}{control of overfitting}
    \end{itemize}
\end{frame}

\subsection{Maximum-likelihood regression}

\begin{frame}
    \frametitle{Maximum-likelihood estimation of model parameters}

	\scriptsize
    \begin{probDef}[Likelihood function]
        For a statistical model characterised by a probability density
        function $p(\mathbf{x}|\theta)$ (or probability mass function
        $P_\theta(X=\mathbf{x})$) the likelihood function is a function of the
        parameters $\theta$, $\mathcal{L}(\theta)=p(\mathbf{x}|\theta)$
        (or $\mathcal{L}(\theta)=P_\theta(\mathbf{x})$).
   \end{probDef}

    \begin{probDef}[Maximum likelihood parameters estimates]
        The maximum likelihood parameters estimates are the parameters that
        maximise the likelihood function.

        \begin{align*}
            \theta_{ML}=\argmax_{\theta}\mathcal{L}(\theta)
        \end{align*}
   \end{probDef}

	\normalsize

\end{frame}

\begin{frame}
    \frametitle{Maximum-likelihood estimation for the basis function linear
    regression model}

    \footnotesize
    We seek the parameter $\mathbf{w}_{ML}$ and $\beta_{ML}$ that maximised the following likelihood function

    \begin{align}
        \mathcal{L}(\mathbf{w},\beta)=p(\mathbf{t}|\mathbf{w},\beta)=\mathcal{N}(\mathbf{t}|\boldsymbol{\Phi}\mathbf{w},\beta^{-1}I_N)=\prod_{n=1}^N\mathcal{N}(t_n|\boldsymbol{\phi}^\intercal(x_n)\mathbf{w},\beta^{-1})\label{eq:likelihoodLinearRegression}
    \end{align}

    They are

    \begin{align}
        \mathbf{w}_{ML}&=(\boldsymbol{\Phi}^\intercal\boldsymbol{\Phi})^{-1}\boldsymbol{\Phi}^\intercal\mathbf{t}\label{eq:wML}\\
        \frac{1}{\beta_{ML}}&=\frac{1}{N}\sum_{n=1}^N(t_n-\phi(\mathbf{x}_n)^\intercal\mathbf{w}_{ML})^2\label{eq:betaML}
    \end{align}

	\begin{itemize}

		\item first regression method that assumes random observations

		\item if the likelihood function is assumed to be Normal,
		maximum-likelihood and least-squares coefficients estimates are equal.

	\end{itemize}

    \normalsize

\end{frame}

\begin{frame}
    \frametitle{Exercise}

    \begin{probExercise}
        Derive the formulas for the maximum likelihood estimates of the
        coefficients, $\mathbf{w}$, and noise precision, $\beta$, of the basis
        functions linear regression model given in Eqs.~\ref{eq:wML}
        and~\ref{eq:betaML}.
    \end{probExercise}

\end{frame}

\subsection{Bayesian linear regression}

\begin{frame}
    \frametitle{Bayesian linear regression: motivation}

	\begin{itemize}
		\item elegant,
		\item naturally allows online regression,
		\item does not require cross-validation for model selection,
		\item it is the first step to more complex Bayesian modelling.
	\end{itemize}

\end{frame}

\subsubsection{Batch Bayesian linear regression}

\begin{frame}
    \frametitle{Batch Bayesian linear regression: posterior distribution of parameters}

	\scriptsize
	In Bayesian linear regression we seek the posterior distribution of the
weights of the linear regression model, $\mathbf{w}$, given the observations, which
is proportional to the product of the likelihood function,
$p(\mathbf{t}|\mathbf{w})$, and the prior, $p(\mathbf{w})$; i.e., 

	\begin{align}
		p(\mathbf{w}|\mathbf{t})\propto
        p(\mathbf{t}|\mathbf{w})p(\mathbf{w})\label{eq:priorLinearRegression}
	\end{align}

	To calculate this posterior below we use the likelihood function defined in
Eq.~\ref{eq:likelihoodLinearRegression} and the following prior

	\begin{align*}
		p(\mathbf{w})=\mathcal{N}(\mathbf{w}|\mathbf{0},\alpha^{-1}\mathbf{I})
	\end{align*}

	Using the expression of the conditional of the Linear Gaussian model,
Eq.~\ref{eq:conditionalLinearGaussianModel}, we obtain

	\begin{align}
		p(\mathbf{w}|\mathbf{t})&=\mathcal{N}(\mathbf{w}|\mathbf{m}_N,\mathbf{S}_N)\nonumber\\
		\mathbf{m}_N&=\beta\mathbf{S}_N\boldsymbol{\Phi}^\intercal\mathbf{t}\label{eq:blrPosteriorMean}\\
		\mathbf{S}_N^{-1}&=\alpha\mathbf{I}+\beta\boldsymbol{\Phi}^\intercal\boldsymbol{\Phi}\label{eq:blrPosteriorCov}
	\end{align}

	\normalsize
\end{frame}

\begin{frame}
    \frametitle{Batch Bayesian linear regression: exercise}

    \scriptsize
    \begin{probExercise}
		Derive the formulas for the Bayesian posterior mean
(Eq.~\ref{eq:blrPosteriorMean}) and covariance (Eq.~\ref{eq:blrPosteriorCov})
of the basis function linear regression model.
    \end{probExercise}

    \begin{probExercise}
        Show that

        \begin{align}
            log
            \log p(\mathbf{w}|\boldsymbol{t})&=K-\frac{\beta}{2}||\mathbf{t}-\boldsymbol{\Phi}\mathbf{w}||_2^2-\frac{\alpha}{2}||\mathbf{w}||_2^2
        \end{align}

        Therefore, the maximum-a-posteriori parameters of the basis function
        linear regression model are the solution of the regularised
        least-squares problem with $\lambda=\alpha/\beta$.

        Note that, as we will show next, Bayesian linear regression uses the
        full posterior of the parameters to make predictions or to do model
        selection, and not just the maximum-a-posteriori parameters.

    \end{probExercise}
    \normalsize

\end{frame}

\begin{frame}
    \frametitle{Batch Bayesian linear regression: demo code}
    Available \href{https://joacorapela.github.io/gcnuBridging2023/auto\_examples/bayesianLinearRegression/plotBatchBayesianLinearRegression.html\#sphx-glr-auto-examples-bayesianlinearregression-plotbatchbayesianlinearregression-py}{here}
\end{frame}

\subsubsection{Online Bayesian linear regression}

\begin{frame}
    \frametitle{Online Bayesian linear regression: recursive update of posterior distribution of parameters}
	\scriptsize
	\begin{claim}[recursive update]
		If the observations, $\{\mathbf{t}_1,\ldots,\mathbf{t}_n,\dots\}$, are linearly independent when conditioned on the model parameters, $\boldsymbol{\theta}$, then for any $n\in\mathbb{N}$
		\begin{align}
			p(\boldsymbol{\theta}|\mathbf{t}_1,\ldots,\mathbf{t}_n)=K\ p(\mathbf{t}_n|\boldsymbol{\theta})p(\boldsymbol{\theta}|\mathbf{t}_1,\ldots,\mathbf{t}_{n-1})
		\end{align}
		where $K$ is a quantity that does not depend on $\boldsymbol{\theta}$.
	\end{claim}
	\normalsize
\end{frame}

\begin{frame}
    \frametitle{Online Bayesian linear regression: recursive update of posterior distribution of parameters}
	\tiny
	\begin{proof}
		By induction on $H_n: p(\boldsymbol{\theta}|\mathbf{t}_1,\ldots,\mathbf{t}_n)=K\ p(\mathbf{t}_n|\boldsymbol{\theta})p(\boldsymbol{\theta}|\mathbf{t}_1,\ldots,\mathbf{t}_{n-1})$.
		\begin{description}
			\item[$H_1$]
				\begin{align*}
					p(\boldsymbol{\theta}|\mathbf{t}_1)=\frac{p(\boldsymbol{\theta},\mathbf{t}_1)}{p(\mathbf{t}_1)}=\frac{p(\mathbf{t}_1|\boldsymbol{\theta})p(\boldsymbol{\theta})}{p(\mathbf{t}_1)}=K\ p(\mathbf{t}_1|\boldsymbol{\theta})p(\boldsymbol{\theta})
				\end{align*}
			\item[$H_n\rightarrow H_{n+1}$]
				\begin{align*}
					p(\boldsymbol{\theta}|\mathbf{t}_1,\ldots,\mathbf{t}_{n+1})&=\frac{p(\boldsymbol{\theta},\mathbf{t}_1,\ldots,\mathbf{t}_{n+1})}{p(\mathbf{t}_1,\ldots,\mathbf{t}_{n+1})}\\
                                                                               &=\frac{p(\mathbf{t}_{n+1}|\boldsymbol{\theta},\mathbf{t}_1,\ldots,\mathbf{t}_n)p(\boldsymbol{\theta},\mathbf{t}_1,\ldots,\mathbf{t}_n)}{p(\mathbf{t}_1\ldots,\mathbf{t}_{n+1})}\\
                                                                               &=\frac{p(\mathbf{t}_{n+1}|\boldsymbol{\theta})p(\boldsymbol{\theta},\mathbf{t}_1,\ldots,\mathbf{t}_n)}{p(\mathbf{t}_1\ldots,\mathbf{t}_{n+1})}\\
                                                                               &=\frac{p(\mathbf{t}_{n+1}|\boldsymbol{\theta})p(\boldsymbol{\theta}|\mathbf{t}_1,\ldots,\mathbf{t}_n)p(\mathbf{t}_1,\ldots,\mathbf{t}_n)}{p(\mathbf{t}_1\ldots,\mathbf{t}_{n+1})}\\
                                                                               &=K\ p(\mathbf{t}_{n+1}|\boldsymbol{\theta})p(\boldsymbol{\theta}|\mathbf{t}_1,\ldots,\mathbf{t}_n)
				\end{align*}
				Note: the third equality above holds because the observations are assumed to be conditional independent given the parameters.
		\end{description}
	\end{proof}
	\normalsize
\end{frame}

\begin{frame}
    \frametitle{Conjugate priors}

    \scriptsize
    Above we showed that, if observations are independent, for the basis
    functions linear regression model

    \begin{align}
        p(\mathbf{w}|\mathbf{t}_1)&\propto
        p(\mathbf{t}_1|\mathbf{w})p(\mathbf{w})\label{eq:conjPrioTmp1}\\
        p(\mathbf{w}|\mathbf{t}_1,\ldots,\mathbf{t}_{n+1})&\propto p(\mathbf{t}_{n+1}|\mathbf{w})p(\mathbf{w}|\mathbf{t}_1,\ldots,\mathbf{t}_n)\label{eq:conjPrioTmp2}
    \end{align}

    It would be helpful to choose a prior $p(\mathbf{w})$ in Eq.~\ref{eq:conjPrioTmp1} such that, for the
    likelihood $p(\mathbf{t}_1|\mathbf{w})$, the posterior
    $p(\mathbf{w}|\mathbf{t}_1)$ has the same functional form at the prior.

    Then, the posterior in Eq.~\ref{eq:conjPrioTmp2} will have the same
    functional form as the``prior''
    $p(\mathbf{w}|\mathbf{t}_1,\ldots,\mathbf{t}_n)$ in the same equation.

    Thus, all posteriors will have the same functional form as the prior
    $p(\mathbf{w})$.

    \begin{probDef}[Conjugate prior]
        If the posterior distribution, $p(\theta|x)$, is in the same
        probability distribution family as the prior probability distribution,
        $p(\theta)$, the prior is called a conjugate prior for the likelihood
        function $p(x|\theta)$.
    \end{probDef}

    Below we prove that the prior we chose for the coefficients of the basis
    function linear regression model, Eq.~\ref{eq:priorLinearRegression}, is a
    conjugate prior for the likelihood function of this model,
    Eq.~\ref{eq:likelihoodLinearRegression}.

    \normalsize

\end{frame}

\begin{frame}
    \frametitle{Conjugate prior for the coefficients of the basis functions
    linear regression model}

    \scriptsize
    \begin{claim}
        If 

        \begin{align}
            P(\mathbf{w}|\mathbf{t}_1,\ldots,\mathbf{t}_n)&=\mathcal{N}(\mathbf{w}|\mathbf{m}_n,\mathbf{S}_n)\label{eq:conjPriorPrior}\\
            P(\mathbf{t}_{n+1}|\mathbf{w})&=\mathcal{N}(\mathbf{t}_{n+1}|\boldsymbol{\Phi}\mathbf{w},\beta^{-1}\mathbf{I})\label{eq:conjPriorLike}
        \end{align}

        then

        \begin{align*}
            P(\mathbf{w}|\mathbf{t}_1,\ldots,\mathbf{t}_{n+1})=\mathcal{N}(\mathbf{w}|\mathbf{m}_{n+1},\mathbf{S}_{n+1})
        \end{align*}

        with

        \begin{align}
            S_{n+1}=\ &S_n-(\beta^{-1}+\phi(\mathbf{x}_{n+1})^\intercal S_n\phi(\mathbf{x}_{n+1}))^{-1}S_n\phi(\mathbf{x}_{n+1})\phi(\mathbf{x}_{n+1})^\intercal
            S_n\label{eq:Sn+1}\\
            m_{n+1}=\ &\beta t_{n+1}S_{n+1}\phi(\mathbf{x}_{n+1})+\mathbf{m}_n-\nonumber\\
                      &(\beta^{-1}+\phi(\mathbf{x}_{n+1})^\intercal
                      S_n\phi(\mathbf{x}_{n+1}))^{-1}\phi(x_{n+1})^\intercal\mathbf{m}_nS_n\phi(x_{n+1})\label{eq:mn+1}
        \end{align}
        \label{claim:conjPriorForBayesianLinearRegression}
    \end{claim}
    \normalsize
\end{frame}

\begin{frame}
    \frametitle{Conjugate prior for the coefficients of the basis functions
    linear regression model}

    \scriptsize
    In the proof below we will use the following lemma.

    \begin{lemma}[Matrix inversion lemma]
        If $A\in\mathbb{R}^{N\times N}$, $U,V\in\mathbf{R}^{N\times M}$ and
        $C\in\mathbb{R}^{M\times M}$ then
        \begin{align*}
            (A+UCV^\intercal)^{-1}=A^{-1}-A^{-1}U(C^{-1}-VA^{-1}U^\intercal)^{-1}V^\intercal A^{-1}
        \end{align*}
    \end{lemma}
    \begin{proof}[Proof for Claim~\ref{claim:conjPriorForBayesianLinearRegression}]
        Using the formula for the conditional of the linear Gaussian model,
        Eq.~\ref{eq:conditionalLinearGaussianModel}, with the expression of the prior,
        Eq.~\ref{eq:conjPriorPrior}, and
        likelihood, Eq.~\ref{eq:conjPriorLike}, we obtain

        \begin{align}
            S_{n+1}&=(S_n^{-1}+\beta
            \phi(\mathbf{x}_{n+1})\phi(\mathbf{x}_{n+1}^\intercal))^{-1}\label{eq:Sn+1Tmp}\\
            \mathbf{m}_{n+1}&=S_{n+1}(\beta t_{n+1}\phi(\mathbf{x}_{n+1})+S_n^{-1}\mathbf{m}_n)\label{eq:mn+1Tmp}
        \end{align}

        Note that Eq.~\ref{eq:Sn+1Tmp} requires the
        inversion and $N\times N$
        matrix, which has a complexity of $\mathcal{O}(N^3)$. We can avoid this
        inversion by using the matrix inversion lemma (with $A=S_n^{-1},
        U=V=\phi(\mathbf{x}_{n+1}), C=\beta$), yielding Eq.~\ref{eq:Sn+1}.

		\phantom\qedhere
    \end{proof}
    \normalsize
\end{frame}


\begin{frame}
    \frametitle{Conjugate prior for the coefficients of the basis functions
    linear regression model}

    \scriptsize
    \begin{proof}
        Eq.~\ref{eq:mn+1Tmp} also requires the inversion of an $N\times N$
        matrix. We can avoid this inversion as follows. From
        Eq.~\ref{eq:mn+1Tmp}

        \begin{align}
            \mathbf{m}_{n+1}&=\beta t_{n+1}S_{n+1}\phi(\mathbf{x}_{n+1})+S_{n+1}S_n^{-1}\mathbf{m}_n\label{eq:mn+1Tmp2}
        \end{align}

        Now we can replace the expression of $S_{n+1}$ given in
        Eq.~\ref{eq:Sn+1} into
        Eq.~\ref{eq:mn+1Tmp2}

        \begin{align*}
            \mathbf{m}_{n+1}=\ &\beta t_{n+1}S_{n+1}\phi(\mathbf{x}_{n+1})+\\
                               &(S_n-(\beta^{-1}+\phi(\mathbf{x}_{n+1})^\intercal S_n\phi(\mathbf{x}_{n+1}))^{-1}S_n\phi(\mathbf{x}_{n+1})\phi(\mathbf{x}_{n+1})^\intercal)S_n^{-1}\mathbf{m}_n\\
                            =\ &\beta t_{n+1}S_{n+1}\phi(\mathbf{x}_{n+1})+\\
                               &(I_n-(\beta^{-1}+\phi(\mathbf{x}_{n+1})^\intercal \phi(\mathbf{x}_{n+1}))^{-1}S_n\phi(\mathbf{x}_{n+1})\phi(\mathbf{x}_{n+1})^\intercal)\mathbf{m}_n\\
                            =\ &\beta t_{n+1}S_{n+1}\phi(\mathbf{x}_{n+1})+\\
                               &\mathbf{m}_n-(\beta^{-1}+\phi(\mathbf{x}_{n+1})^\intercal \phi(\mathbf{x}_{n+1}))^{-1}S_n\phi(\mathbf{x}_{n+1})\phi(\mathbf{x}_{n+1})^\intercal\mathbf{m}_n
        \end{align*}

    \end{proof}
    \normalsize
\end{frame}

\begin{frame}
    \frametitle{Conjugate prior for the coefficients of the basis functions
    linear regression model}

    \scriptsize
    Note that Eqs.~\ref{eq:Sn+1Tmp} and~\ref{eq:mn+1Tmp} both required the inversion
    of an $N\times N$ matrix, but Eqs.~\ref{eq:Sn+1} and~\ref{eq:mn+1} only
    require the inversion of scalars.

    \vspace{0.3in}

    Python code implementing online Bayesian regression can be found
    \href{https://joacorapela.github.io/gcnuBridging2023/auto\_examples/bayesianLinearRegression/plotOnlineBayesianLinearRegression.html\#sphx-glr-auto-examples-bayesianlinearregression-plotonlinebayesianlinearregression-py}{here}.

    \normalsize

\end{frame}

\subsubsection{Bayesian predictions}

\begin{frame}
    \frametitle{Bayesian predictions}

    \scriptsize
    \begin{description}
        \item[least squares]
            \begin{align*}
                t_{new}=\phi(\mathbf{x}_{new})^\intercal\mathbf{w}_{LS}
            \end{align*}
        \item[Bayesian]
            \begin{align*}
                p(t_{new}|\mathbf{t},\alpha,\beta)&=\int p(t_{new},\mathbf{w}|\mathbf{t},\alpha,\beta)d\mathbf{w}\\
                                                  &=\int p(t_{new}|\mathbf{w},\beta)p(\mathbf{w}|\mathbf{t},\alpha,\beta)d\mathbf{w}\\
                                                  % &=\int \mathcal{N}(t_{new}|\phi(\mathbf{x}_{new})^\intercal\mathbf{w},\beta^{-1}\mathbf{I})\mathcal{N}(\mathbf{w}|\mathbf{m}_N,S_N)d\mathbf{w}
            \end{align*}
    \end{description}
    \begin{probExercise}
        Derive the close form solution of the Bayesian predictive distribution.
    \end{probExercise}
    \normalsize
\end{frame}

\begin{frame}
    \frametitle{Code for Bayesian predictions}

    Available \href{https://joacorapela.github.io/gcnuBridging2023/auto\_examples/bayesianLinearRegression/plotPredictiveDistribution.html\#sphx-glr-auto-examples-bayesianlinearregression-plotpredictivedistribution-py}{here}.

\end{frame}

\subsubsection{Bayesian model comparison}

\begin{frame}
    \frametitle{Model comparison}

    \scriptsize
    We want to compare which of a set of basis function linear regression
    models $\{\mathcal{M}_1,\ldots,\mathcal{M}_Q\}$ best fits a given dataset,
    $\mathbf{t}$ without using cross validation. For this, we will compare the
    models evidences or marginal likelihoods:

    \begin{align}
        p(\mathbf{t}|\alpha,\beta)=\int
        p(\mathbf{t},\mathbf{w}|\alpha,\beta)d\mathbf{w}=\int
        p(\mathbf{t}|\mathbf{w},\beta)p(\mathbf{w}|\alpha)d\mathbf{w}\label{eq:modelComparisonTmp1}
    \end{align}

    with $p(\mathbf{t}|\mathbf{w},\beta)$ and $p(\mathbf{w}|\alpha)$ given in
    Eqs.~\ref{eq:likelihoodLinearRegression}
    and~\ref{eq:priorLinearRegression}, respectively.

    \begin{probExercise}
        Show that

        \begin{align*}
            \log p(\mathbf{t}|\alpha,\beta)=\frac{M}{2}\log\alpha+\frac{N}{2}\log\beta-E(\mathbf{m}_N)-\frac{1}{2}\log|\mathbf{A}|-\frac{N}{2}\log(2\pi)
        \end{align*}

        where
        $E(\mathbf{m}_N)=\frac{\beta}{2}||\mathbf{t}-\boldsymbol{\Phi}\mathbf{m}_n||^2+\frac{\alpha}{2}\mathbf{m}_N^\intercal\mathbf{m}_N$,
        $A=\alpha\mathbf{I}+\beta\boldsymbol{\Phi}^\intercal\boldsymbol{\Phi}$
        and $\mathbf{m}_N$ is the mean of
        $p(\mathbf{w}|\mathbf{t},\alpha,\beta)$

        \vspace{0.5cm}
        Hint: Integrate Eq.~\ref{eq:modelComparisonTmp1} using
        Eq.~\ref{eq:marginalLinearGaussianModel}, or by completing the squares.

        \end{probExercise}
    \normalsize
\end{frame}

\begin{frame}
    \frametitle{Code for Bayesian model comparison}

    Available \href{https://joacorapela.github.io/gcnuBridging2023/auto\_examples/bayesianLinearRegression/plotModelsEvidences.html\#sphx-glr-auto-examples-bayesianlinearregression-plotmodelsevidences-py}{here}.

\end{frame}

\subsection{Practical: decoding stimuli intensity from spiking activity of
retinal ganglion cells}

\begin{frame}
    \frametitle{Decoding stimuli intensity from spiking activity of
retinal ganglion cells}

    Using the dataset provided for this
    \href{https://compneuro.neuromatch.io/tutorials/W1D3_GeneralizedLinearModels/student/W1D3_Tutorial1.html}{tutorial},

    \begin{enumerate}[a]

        \item determine if a basis function linear regression model is adequate
            to characterize this dataset,

        \item use Bayesian model comparison to test if the 200~ms decoding time window
    used in the tutorial is statistically optimal,

        \item test if a nonlinear decoder using Gaussian basis functions
            outperforms a linear decoder using the identity basis function.

    \end{enumerate}

\end{frame}

\begin{frame}
    \frametitle{References}

    \tiny{
        \bibliographystyle{apalike}
        \bibliography{probability,informationTheory,machineLearning,gaussianProcesses,latentsVariablesModels,linearDynamicalSystems,numericalMethods}
    }
\end{frame}

\end{document}

    \end{probExercise}
    We integrate Eq.~\ref{eq:modelComparison} using the expression for the
    marginal of the linear Gaussian model,
    Eq.~\ref{eq:marginalLinearGaussianModel}, obtaining

    \begin{align}
        p(\mathbf{t}|\alpha,\beta)=\mathcal{N}(\mathbf{t}|\mathbf{0},\alpha^{-1}\boldsymbol{\phi}\boldsymbol{\Phi}^\intercal+\beta^{-1}\mathbf{I}_N)
    \end{align}

    \normalsize
\end{frame}

\begin{frame}
    \frametitle{References}

    \tiny{
        \bibliographystyle{apalike}
        \bibliography{probability,informationTheory,machineLearning,gaussianProcesses,latentsVariablesModels,linearDynamicalSystems,numericalMethods}
    }
\end{frame}

\end{document}

