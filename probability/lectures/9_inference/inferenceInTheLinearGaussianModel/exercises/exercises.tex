\documentclass[12pt]{article}

\usepackage{natbib}
\usepackage{apalike}
\usepackage{amsmath,amssymb}
\usepackage[colorlinks=true]{hyperref}
\usepackage{url}
\usepackage[shortlabels]{enumitem}
\usepackage{verbatim}
\usepackage{amsthm}

\newtheorem{claim}{Claim}

\begin{document}

\title{Exercises: unsupervised inference with iid observations and the linear Gaussian model}
\author{Joaqu\'{i}n Rapela\thanks{j.rapela@ucl.ac.uk}}

\maketitle

\section{Inferring location of a static submarine from its sonar
measurements}

This exercise is an extension of Example 3.3.4 from
another great book in machine learning, \citet{murphyIntro22}\footnote{See this related
\href{https://github.com/probml/pml-book/issues/510}{discussion} and its
\href{https://github.com/probml/pml-book/issues/512}{follow-up}.}.
%
A static submarine is located in a 2D planar surface deep in the sea. A
priori, we model its unknown location with a 2D random variable
$\mathbf{z}$ with 

\begin{align}
    p(\mathbf{z})=\mathcal{N}\left(\mathbf{z}|\mu_z,\Sigma_z\right)
\end{align}

We obtain noisy measurements of the location of the submarine with a sonar.
We represent a 2D sonar measurement with random variable $\mathbf{y}_n$,
with 

\begin{align}
    p(\mathbf{y}_n|\mathbf{z})=\mathcal{N}(y_n|\mathbf{z}, \Sigma_y)
\end{align}

\begin{enumerate}[(a)]

    \item Sample 100 a priory locations of the submarine
        (i.e., $\mathbf{z}_1,\ldots,\mathbf{z}_{100}$) using
        a mean $\mu_z=[2,3]^\intercal$, a standard deviation along the
        horizontal direction $\sigma_{\mathbf{z}x}=1.0$, a standard
        deviation along the vertical direction $\sigma_{\mathbf{z}y}=1.0$,
        and a correlation coefficient between the vertical and horizontal
        directions $\rho_{\mathbf{z}}=-0.8$.

        Plot $\mu_z$, the samples of the a-priori submarine location. and the
        ellipse containing 95\% of samples of a-priori submarine locations. We
        call this ellipse the 95\% prediction ellipse for samples of the
        a-priori submarine location.

        You may want to complete the script
        \href{https://github.com/joacorapela/gcnuBridging2023/blob/master/code/scripts/probability/unsupervisedInferenceInTheLinearGaussianModel/doExSubmarine_a.py}{doExSubmarine\_a.py}
        to address this item.

    \item Select the submarine location $\mathbf{z}_1$ generated in
        the previous item. Sample $N=5$ sonar measurements, assuming the
        submarine is at location $\mathbf{z}_1$ (i.e., sample from
        $p(\mathbf{y}|\mathbf{z}_1)$ to obtain
        $\mathbf{y}_1,\ldots,\mathbf{y}_N$). Use a standard deviation of
        1.0 for the measurement noise along the horizontal and vertical
        directions, and a correlation coefficient between the vertical and
        horizontal directions $\rho_{\mathbf{y}}=0.8$.

        Plot $\mathbf{z}_1$, the sonar measurements samples. and the 95\%
        prediction ellipse for sonar measurement samples.

        You may want to complete the script
        \href{https://github.com/joacorapela/gcnuBridging2023/blob/master/code/scripts/probability/unsupervisedInferenceInTheLinearGaussianModel/doExSubmarine_b.py}{doExSubmarine\_b.py}
        to address this item.

    \item derive a mathematical expression for the posterior of the
        submarine location, given sonar measurements; i.e.,
        $p(\mathbf{z}|\mathbf{y}_1,\ldots,\mathbf{y}_N)$.

        \textit{Hints}: 

        \begin{itemize}

            \item The posterior of the submarine location given sonar
                measurements is proportional to the joint distribution of
                the submarine location and sonar measurements; i.e.,
                $p(\mathbf{z}|\mathbf{y}_1,\ldots\mathbf{y}_N)=\frac{p(\mathbf{y}_1,\ldots,\mathbf{y}_N,\mathbf{z})}{p(\mathbf{y}_1,\ldots,\mathbf{y}_N)}=K\,p(\mathbf{y}_1,\ldots,\mathbf{y}_N,\mathbf{z})$,
                where $K$ is a value that does not depend on $\mathbf{z}$. Thus, to
                obtain the posterior we can just keep the terms of the
                joint that depend on $\mathbf{z}$ and normalise the
                resulting expression to integrate to one.

            \item The joint is the product of the likelihood and the prior;
                i.e.,
                $p(\mathbf{y}_1,\ldots\mathbf{y}_N,\mathbf{z})=p(\mathbf{y}_1,\ldots,\mathbf{y}_N|\mathbf{z})p(\mathbf{z})$.
                Thus, to keep the terms of the joint that depend on
                $\mathbf{z}$, we can just keep the term of the
                likelihood that depend on $\mathbf{z}$ and combine the
                result with the prior.

            \item As shown in Claim.~\ref{claim:likelihoodForZ}, the terms of the
                likelihood that depend on $\mathbf{z}$ are proportional to
                a Gaussian distribution with mean $\mathbf{z}$ and
                covariance $\frac{1}{N}\Sigma$; i.e., 
                $p(\mathbf{y}_1,\ldots,\mathbf{y}_N|\mathbf{z})=K\,\mathcal{N}(\bar{\mathbf{y}}|\mathbf{z},\frac{1}{N}\Sigma_y)$,
                where $K$ is a value that does not depend on $\mathbf{z}$.

            \item From the previous arguments, to obtain the posterior of
                $\mathbf{z}$ we can multiply
                $\mathcal{N}(\bar{\mathbf{y}}|\mathbf{z},\frac{1}{N}\Sigma_y)$
                with the prior
                $p(\mathbf{z})=\mathcal{N}(\mathbf{z}|\mu_z,\Sigma_z)$ and
                normalise the result. To do this we can use the expression
                for the posterior of the linear Gaussian model described in
                class.

        \end{itemize}

    \item plot the sample mean of the measurements, the 95\% prediction ellipse for
        sample means of the measurements, the mean of the posterior distribution and the 95\%
        prediction ellipse for samples from the posterior.

        You may want to complete the script
        \href{https://github.com/joacorapela/gcnuBridging2023/blob/master/code/scripts/probability/unsupervisedInferenceInTheLinearGaussianModel/doExSubmarine_d.py}{doExSubmarine\_d.py}
        to address this item.

    \item repeat (b) and (d) with $N\in\{1,10,100,1000\}$ measurements, and show
        the plots generated in (d).  How do the posterior and sample mean
        estimates change as $N$ increases?

    \item write expressions of the posterior mean and covariances to show that:

        \begin{itemize}

            \item as the number of measurements increases, the relative
                contribution of the prior to estimates of the posterior
                mean and covariance decreases,

            \item in the limit when the number of measurements approaches
                infinity, the posterior covariance approaches zero and the
                posterior mean approaches the measurements sample mean.
                That is, for an infinite number of measurements, the
                posterior estimate becomes deterministic and the
                contribution of the prior to this estimate disappears.

        \end{itemize}

        Can you see these points in the previous simulations?

\end{enumerate}

\pagebreak

\begin{claim}
    If
    $P(\mathbf{y}_i|\mathbf{z})=\mathcal{N}\left(\mathbf{y}_i|\mathbf{z},\Sigma\right)$,
    $i=1,\ldots,N$, 

    and
    $P(\mathbf{y}_1,\ldots,\mathbf{y}_N|\mathbf{z})=\prod_{i=1}^N
    P(\mathbf{y}_i|\mathbf{z})$, 

    then
    $P(\mathbf{y}_1,\ldots,\mathbf{y}_N|\mathbf{z})=K\mathcal{N}(\bar{\mathbf{y}}_N|\mathbf{z},\frac{1}{N}\Sigma)$

    where $K$ is a value unrelated to $\mathbf{z}$.
    \label{claim:likelihoodForZ}
\end{claim}

\begin{proof}
    By induction:
    $P_n=P(\mathbf{y}_1,\ldots,\mathbf{y}_n|\mathbf{z})=K\mathcal{N}(\bar{\mathbf{y}}_n|\mathbf{z},\frac{1}{n}\Sigma)$

    $P_1$:
    \begin{align*}
        P(\mathbf{y}_1|\mathbf{z})=\mathcal{N}(\mathbf{y}_1|\mathbf{z},\Sigma)=\mathcal{N}(\bar{\mathbf{y}}_1|\mathbf{z},\frac{1}{1}\Sigma)
    \end{align*}

    $P_n\rightarrow P_{n+1}$:

    \begin{align*}
        P(\mathbf{y}_1,\ldots,\mathbf{y}_n,\mathbf{y}_{n+1}|\mathbf{z})&=\prod_{i=1}^{n+1} P(\mathbf{y}_i|\mathbf{z})\\
                  &=P(\mathbf{y}_1,\ldots,\mathbf{y}_n|\mathbf{z})P(\mathbf{y}_{n+1}|\mathbf{z})\\
                  &=\mathcal{N}(\bar{\mathbf{y}}_n|\mathbf{z},\frac{1}{n}\Sigma)\mathcal{N}(\mathbf{y}_{n+1}|\mathbf{z},\Sigma)
    \end{align*}

    \noindent then

    \begin{align*}
        % \log P(\mathbf{y}_1,\ldots,\mathbf{y}_n,\mathbf{y}_{n+1}|\mathbf{z})&=K-\frac{1}{2}(\bar{\mathbf{y}}_n-\mathbf{z})^\intercal n\Sigma^{-1}(\bar{\mathbf{y}}_n-\mathbf{z})- \frac{1}{2}({\mathbf{y}_{n+1}-\mathbf{z})^\intercal \Sigma^{-1}(\mathbf{y}_{n+1}-\mathbf{z}) 
        % 1 + 1  = 2
        \log
        P(\mathbf{y}_1,\ldots,\mathbf{y}_n,\mathbf{y}_{n+1}|\mathbf{z})=&K-\frac{1}{2}(\bar{\mathbf{y}}_n-\mathbf{z})^\intercal
        n\Sigma^{-1}(\bar{\mathbf{y}}_n-\mathbf{z})-\\
        &\frac{1}{2}(\mathbf{y}_{n+1}-\mathbf{z})^\intercal\Sigma^{-1}(\mathbf{y}_{n+1}-\mathbf{z})\\
        =&K_1-\frac{1}{2}\left(\mathbf{z}^\intercal(n+1)\Sigma^{-1}\mathbf{z}-2\mathbf{z}^\intercal n\Sigma^{-1}\bar{\mathbf{y}}_n-2\mathbf{z}^\intercal\Sigma^{-1}\mathbf{y}_{n+1}\right)\\
        =&K_1-\frac{1}{2}\left(\mathbf{z}^\intercal(n+1)\Sigma^{-1}\mathbf{
            z}-2\mathbf{z}^\intercal \Sigma^{-1}\sum_{i=1}^n\mathbf{y}_i-2\mathbf{z}^\intercal\Sigma^{-1}\mathbf{y}_{n+1}\right)\\
        =&K_1-\frac{1}{2}\left(\mathbf{z}^\intercal(n+1)\Sigma^{-1}\mathbf{
            z}-2\mathbf{z}^\intercal\Sigma^{-1}\sum_{i=1}^{n+1}\mathbf{y}_i\right)\\
        =&K_1-\frac{1}{2}\left(\mathbf{z}^\intercal(n+1)\Sigma^{-1}\mathbf{
            z}-2\mathbf{z}^\intercal(n+1)\Sigma^{-1}\bar{\mathbf{y}}_{n+1}\right)\\
    \end{align*}

    Therefore

    \begin{align*}
        P(\mathbf{y}_1,\ldots,\mathbf{y}_n,\mathbf{y}_{n+1}|\mathbf{z})=&K_2\,\mathcal{N}\left(\mathbf{z}\left|\bar{\mathbf{y}}_{n+1},\frac{1}{n+1}\Sigma\right.\right)=K_2\,\mathcal{N}\left(\bar{\mathbf{y}}_{n+1}\left|\mathbf{z},\frac{1}{n+1}\Sigma\right.\right)
    \end{align*}

\end{proof}

\bibliographystyle{apalike}
\bibliography{machineLearning}

\end{document}
