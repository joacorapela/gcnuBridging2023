\documentclass[12pt]{article}

\usepackage{amsmath,amssymb}
% \usepackage[colorlinks=true]{hyperref}
% \usepackage[shortlabels]{enumitem}
% \usepackage{verbatim}
% \usepackage{amsthm}
\usepackage{graphicx}
\usepackage{float}
\usepackage[colorlinks=true]{hyperref}
\usepackage{listings}
\usepackage{tikz}
\usepackage{natbib}
\usepackage{apalike}

\def\figsdir{../../../../../../code/scripts/probability/multivariateGaussians/inferenceInTheLinearGaussianModel/figures/}

\newcommand\nSamples{{3,10,50,100,1000}}
\def\nNSamples{5}

\begin{document}

\title{Exercises: inference in the linear Gaussian model}
\author{Joaqu\'{i}n Rapela\thanks{j.rapela@ucl.ac.uk}}

\maketitle

\section{Inferring location of a static submarine from its sonar
measurements}

\subsection*{(a)} The modified code lines appear below and Fig.~\ref{fig:a}
shows the generated submarine samples, and the mean and 95\% confidence ellipse
of the samples probability density function.

\begin{lstlisting}[language=Python]
    sigma_zx = 1.0
    sigma_zy = 2.0
    rho_z = 0.7
    cov_z_11 = sigma_zx**2
    cov_z_12 = rho_z*sigma_zx*sigma_zy
    cov_z_21 = rho_z*sigma_zx*sigma_zy
    cov_z_22 = sigma_zy**2
\end{lstlisting}

\begin{center}
    \begin{figure}[H]
        \includegraphics[width=6in]{\figsdir /submarine_samples_N100.png}

        \caption{100 a-priori samples of the submarine location (dots), and the
        mean (cross) and 95\% confidence ellipse (line) of the samples
        probability density function.}

        \label{fig:a}
    \end{figure}
\end{center}

\subsection*{(b)} The modified code lines appear below and Fig.~\ref{fig:b}
shows the generated measurement samples, and the mean and 95\% confidence
ellipse of the samples probability density function.

\begin{lstlisting}[language=Python]
    sigma_y_x = 1.0
    sigma_y_y = 1.0
    rho_y = 0.0
    cov_y_11 = sigma_y_x**2
    cov_y_12 = rho_y*sigma_y_x*sigma_y_y
    cov_y_21 = rho_y*sigma_y_x*sigma_y_y
    cov_y_22 = sigma_y_y**2
\end{lstlisting}

\begin{center}
    \begin{figure}[H]
        \includegraphics[width=6in]{\figsdir /measurements_samples_N5.png}

        \caption{5 noisy measurements of of the submarine location (dots), and
        the mean ($\mathbf{z}_1$, cross) and 95\% confidence ellipse (line) of
        the measurements probability density function.}

        \label{fig:b}
    \end{figure}
\end{center}

\subsection*{(c)} 

\begin{align}
    p(\mathbf{z}|\mathbf{y}_1,\ldots,\mathbf{y}_N)&=K_1\ p(\mathbf{z},\mathbf{y}_1,\ldots,\mathbf{y}_N)\nonumber\\
                                                  &=K_1\ p(\mathbf{y}_1,\ldots,\mathbf{y}_N|\mathbf{z})\ p(\mathbf{z})\nonumber\\
                                                  &=K_2\
                                                  \mathcal{N}\left(\bar{\mathbf{y}}\left|\mathbf{z},\frac{1}{N}\Sigma_y\right.\right)\mathcal{N}\left(\mathbf{z}|\mu_z,\Sigma_z\right)\label{eq:posZLine3}
%                                                   &=K_2\ p(\bar{\mathbf{y}}|\mathbf{z})\ p(\mathbf{z})
\end{align}

\noindent where $K_1$ is a constant that does not depend on $\mathbf{z}$ and
Eq.~\ref{eq:posZLine3} follows from Claim~1 in the exercise statement. In the
right-hand side of Eq.~\ref{eq:posZLine3} we recognize a linear Gaussian model
(i.e., $\bar{\mathbf{y}}$ and $\mathbf{z}$ are Gaussian random variables and
the mean of $\bar{\mathbf{y}}$ depends linearly on $\mathbf{z}$).

Defining
$p(\bar{\mathbf{y}}|\mathbf{z})=\mathcal{N}\left(\bar{\mathbf{y}}\left|\mathbf{z},\frac{1}{N}\Sigma_y\right.\right)$
and $p(\mathbf{z})=\mathcal{N}\left(\mathbf{z}|\mu_z,\Sigma_z\right)$, because
the right-hand side of Eq.~\ref{eq:posZLine3} equals a probability density
function on $\mathbf{z}$, this right-hand side should be
$p(\mathbf{z}|\bar{\mathbf{y}})$. To derive a mathematical expression for
$p(\mathbf{z}|\bar{\mathbf{y}})$, we use Eq.~3.37 from~\citet{murphyIntro22}
with $\mathbf{y}=\bar{\mathbf{y}}$, $\mathbf{W}=I$, $\mathbf{b}=\mathbf{0}$,
$\Sigma_y=\frac{1}{N}\Sigma_y$, yielding


\begin{align}
    p(\mathbf{z}|\mathbf{y}_1,\ldots,\mathbf{y}_N)&=p(\mathbf{z}|\bar{\mathbf{y}})=\mathcal{N}(\mathbf{z}|\mu_{z|\bar{y}},\Sigma_{z|\bar{y}})\nonumber\\
    \Sigma^{-1}_{z|\bar{y}}&=\Sigma_z^{-1}+N\Sigma_y^{-1}\label{eq:postCov}\\
    \mu_{z|\bar{y}}&=\Sigma_{z|\bar{y}}\left[N\Sigma_y^{-1}\bar{\mathbf{y}}+\Sigma_z^{-1}\mu_z\right]\label{eq:postMean}
\end{align}

\subsection*{(d)} The modified code lines appear below and Fig.~\ref{fig:d}
plots the mean of the measurements, the mean of the posterior and its 95\%
confidence ellipse.

\begin{lstlisting}[language=Python]
    cov_y_inv = np.linalg.inv(cov_y)
    cov_z_inv = np.linalg.inv(cov_z)
    tmp1 = N * cov_y_inv + cov_z_inv
    tmp2 = N * np.matmul(cov_y_inv, sample_mean_y) + \
        np.matmul(cov_z_inv, mean_z)
    post_mean_z = np.linalg.solve(tmp1, tmp2)
    post_cov_z = np.linalg.inv(tmp1)
    yBar_mean = z
    yBar_cov = 1.0/N*cov_y
\end{lstlisting}

\begin{center}
    \begin{figure}[H]
        \includegraphics[width=6in]{\figsdir /estimates_samples_N5.png}
        \caption{Sample average of 5 noisy measurements (blue cross), 95\%
        confidence ellipse of this average (blue line), mean of the posterior
        distribution (red cross), its 95\% confidence ellipse (red line), and
        submarine location ($\mathbf{z}_1$, red dot)}
        \label{fig:d}
    \end{figure}
\end{center}

\subsection*{(e)} Figs.~\ref{fig:e_3}-\ref{fig:e_100} plot the posterior
estimates computed from an increasing number of measurements.

In these figures we observe that:

\begin{enumerate}

    \item as the number of measurements increases, the posterior mean approaches
        the sample mean, and the sample mean approaches the submarine location
        $\mathbf{z}_1$,

    \item as the number of measurements increases, the 95\% confidence ellipses
        become smaller,

    \item for three measurements (Fig.~\ref{fig:e_3}) the posterior 95\%
        confidence ellipse is tilted, as that of the prior (Fig.~\ref{fig:a},
        $\Sigma_z$ in Eq.~1 of the exercise statement).
        As the number of measurments increases, the posterior 95\% confidence
        ellipses become more and more spherical, as the 95\% confidence ellipse
        of the measurements likelihood (Fig.~\ref{fig:b},
        $\Sigma_y$ in Eq.~2 of the exercise statement).
\end{enumerate}

\foreach \i in {1,...,\nNSamples} {
    \pgfmathtruncatemacro{\nSamplesI}{\i-1}
    \pgfmathtruncatemacro{\nSamplesL}{\nSamples[\nSamplesI]}

    \begin{center}
        \begin{figure}[H]
            \includegraphics[width=6in]{\figsdir /estimates_samples_N\nSamplesL.png}

            \caption{Sample average of \nSamplesL\ noisy measurements (blue
            cross), 95\% confidence ellipse of this average (blue line), mean
            of the posterior distribution (red cross), its 95\% confidence
            ellipse (red line), and submarine location
            ($\mathbf{z}_1$, red dot)}

            \label{fig:e_\nSamplesL}
        \end{figure}
    \end{center}
}

\subsection*{(f)} Eqs~\ref{eq:postMeanRewritten} and~\ref{eq:postCovRewritten}
were obtained by re-arrenging Eqs.~\ref{eq:postMean} and~\ref{eq:postCov} to
more clearly show the behavior of the posterior mean and covariance as $N$
increases to infinity.

\begin{align}
    p(\mathbf{z}|\mathbf{y}_1,\ldots,\mathbf{y}_N)&=\mathcal{N}(\mathbf{z}|\mu_{z|\bar{y}}(N),\Sigma_{z|\bar{y}}(N))\nonumber\\
    \Sigma_{z|\bar{y}}(N)&=\frac{1}{N}\left(\Sigma_y^{-1}+\frac{1}{N}\Sigma_z^{-1}\right)^{-1}\label{eq:postCovRewritten}\\
    \mu_{z|\bar{y}}(N)&=\left(\Sigma_y^{-1}+\frac{1}{N}\Sigma_z^{-1}\right)^{-1}\left[\Sigma_y^{-1}\bar{\mathbf{y}}_N+\frac{1}{N}\Sigma_z^{-1}\mu_z\right]\label{eq:postMeanRewritten}
\end{align}

From Eq.~\ref{eq:postCovRewritten} we observe that as $N$ increases the
contributions of the prior covariance, $\Sigma_z$, to the posterior covariance,
$\Sigma_{z|\bar{y}}(N)$, becomes smaller and smaller, in comparison to the
contribution from the likelihood covariance, $\frac{1}{N}\Sigma_y$. When $N$ is
very large, the contribution of the prior covariance dissapears, the posterior
covariance converges to the likelihood covariance, which becomes zero.

From Eq.~\ref{eq:postMeanRewritten} we observe 

\begin{align}
    \lim_{N\rightarrow\infty}\mu_{z|\bar{y}}(N)&=\Sigma_y\left[\Sigma_y^{-1}\bar{\mathbf{y}}_N\right]=\lim_{N\rightarrow\infty}\bar{\mathbf{y}}_N\label{eq:postMeanAtInfty}
\end{align}

In class we proved that

\begin{align}
    \bar{\mathbf{y}}_N\sim\mathcal{N}(\bar{\mathbf{y}}_N|\mathbf{z}_1,\frac{1}{N}\Sigma_y)\nonumber
\end{align}

\noindent Thus, as $N$ approaches infinity, the variance of $\bar{\mathbf{y}}_N$
becomes zero, and $\bar{\mathbf{y}}_N$ collapses to its mean $\mathbf{z}_1$.
Therefore, as $N$ approaches infinity, both the posterior mean,
Eq.~\ref{eq:postMeanAtInfty}, and sample the mean, become deterministic and
converge to the population mean of the observatons; i.e., $\mathbf{z}_1$.

\bibliographystyle{apalike}
\bibliography{machineLearning}

\end{document}
